# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

google_cloud_project:
  project_id: "${project_id}"
  project_name: "${project_name}"
  project_number: "${project_number}"
  region: "us-central1"

cloud_build:
  project_id: "${project_id}"
  region: "us-central1"
  github:
    owner: "${pipelines_github_owner}"
    repo_name: "${pipelines_github_repo}"
    trigger_branch: "uat"
  build_file: "cloudbuild/pipelines.yaml"
  _REPOSITORY_GCP_PROJECT: "${project_id}"
  _REPOSITORY_NAME: "github_${pipelines_github_owner}_${pipelines_github_repo}"
  _REPOSITORY_BRANCH: "main"
  _GCR_HOSTNAME: "us-central1-docker.pkg.dev"
  _BUILD_REGION: "us-central1"

container:
  builder:
    base:
      from_image: "python:3.7-alpine3.7"
      base_image_name: "base-builder"
      base_image_prefix: "propensity-modeling"
    zetasql:
      from_image: "wbsouza/zetasql-formatter:latest"
      base_image_name: "zetasql-formatter"
      base_image_prefix: "propensity-modeling"
  container_registry_hostname: "us-central1-docker.pkg.dev" 
  container_registry_region: "us-central1"

artifact_registry:
  pipelines_repo:
    name: "pipelines-repo"
    region: "us-central1"
    project_id: "${project_id}"
  pipelines_docker_repo:
    name: "pipelines-docker-repo"
    region: "us-central1"
    project_id: "${project_id}"

dataflow:
  worker_service_account_id: "df-worker"
  worker_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"

vertex_ai:
  components:
    base_image_name: "ma-components"
    base_image_tag: "dev"

  pipelines: 
    project_id: "${project_id}"
    service_account_id: "vertex-pipelines-sa"
    service_account: "vertex-pipelines-sa@${project_id}.iam.gserviceaccount.com"
    region: "us-central1"
    bucket_name: "${project_id}-pipelines"
    root_path: "gs://${project_id}-pipelines/pipelines/"

    feature-creation:
      execution:
        name: "feature-creation-pl"
        job_id_prefix: "feature-creation-pl-"
        experiment_name: "feature-creation-dev"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York */1 * * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "US"
          query_customer_lifetime_value_label: "
            DECLARE input_date DATE;
            DECLARE end_date DATE;
            DECLARE users_added INT64 DEFAULT NULL;
            SET end_date= IFNULL(@input_date, CURRENT_DATE('{date_timezone}'));
            SET input_date= (SELECT DATE_SUB(end_date, INTERVAL 180 DAY));
            CALL `{customer_lifetime_value_label_procedure_name}`(input_date, end_date, users_added);"
          query_purchase_propensity_label: "
            DECLARE input_date DATE;
            DECLARE end_date DATE;
            DECLARE users_added INT64 DEFAULT NULL;
            SET end_date= IFNULL(@input_date, CURRENT_DATE('{date_timezone}'));
            SET input_date= (SELECT DATE_SUB(end_date, INTERVAL 180 DAY));
            CALL `{purchase_propensity_label_procedure_name}`(input_date, end_date, users_added);"
          query_user_dimensions: "
            DECLARE input_date DATE;
            DECLARE end_date DATE;
            DECLARE users_added INT64 DEFAULT NULL;
            SET input_date= IFNULL(@input_date, CURRENT_DATE('{date_timezone}'));
            SET end_date= (SELECT DATE_SUB(input_date, INTERVAL 30 DAY));
            CALL `{user_dimensions_procedure_name}`(input_date, end_date, users_added);"
          query_user_lifetime_dimensions: "
            DECLARE input_date DATE;
            DECLARE end_date DATE;
            DECLARE users_added INT64 DEFAULT NULL;
            SET input_date= IFNULL(@input_date, CURRENT_DATE('{date_timezone}'));
            SET end_date= (SELECT DATE_SUB(input_date, INTERVAL 180 DAY));
            CALL `{user_lifetime_dimensions_procedure_name}`(input_date, end_date, users_added);"
          query_user_lookback_metrics: "
            DECLARE input_date DATE;
            DECLARE end_date DATE;
            DECLARE users_added INT64 DEFAULT NULL;
            SET input_date= IFNULL(@input_date, CURRENT_DATE('{date_timezone}'));
            SET end_date= (SELECT DATE_SUB(input_date, INTERVAL 15 DAY));
            CALL `{user_lookback_metrics_procedure_name}`(input_date, end_date, users_added);"
          query_user_rolling_window_lifetime_metrics: "
            DECLARE input_date DATE;
            DECLARE end_date DATE;
            DECLARE users_added INT64 DEFAULT NULL;
            SET input_date= IFNULL(@input_date, CURRENT_DATE('{date_timezone}'));
            SET end_date= (SELECT DATE_SUB(input_date, INTERVAL 180 DAY));
            CALL `{user_rolling_window_lifetime_metrics_procedure_name}`(input_date, end_date, users_added);"
          query_user_rolling_window_metrics: "
            DECLARE input_date DATE;
            DECLARE end_date DATE;
            DECLARE users_added INT64 DEFAULT NULL;
            SET input_date= IFNULL(@input_date, CURRENT_DATE('{date_timezone}'));
            SET end_date= (SELECT DATE_SUB(input_date, INTERVAL 30 DAY));
            CALL `{user_rolling_window_metrics_procedure_name}`(input_date, end_date, users_added);"
          query_user_scoped_lifetime_metrics: "
            DECLARE input_date DATE;
            DECLARE end_date DATE;
            DECLARE users_added INT64 DEFAULT NULL;
            SET input_date= IFNULL(@input_date, CURRENT_DATE('{date_timezone}'));
            SET end_date= (SELECT DATE_SUB(input_date, INTERVAL 180 DAY));
            CALL `{user_scoped_lifetime_metrics_procedure_name}`input_date, end_date, users_added);"
          query_user_scoped_metrics: "
            DECLARE input_date DATE;
            DECLARE end_date DATE;
            DECLARE users_added INT64 DEFAULT NULL;
            SET input_date= IFNULL(@input_date, CURRENT_DATE('{date_timezone}'));
            SET end_date= (SELECT DATE_SUB(input_date, INTERVAL 30 DAY));
            CALL `{user_scoped_metrics_procedure_name}`input_date, end_date, users_added);"
          query_user_scoped_segmentation_metrics: "
            DECLARE input_date DATE;
            DECLARE end_date DATE;
            DECLARE users_added INT64 DEFAULT NULL;
            SET input_date= IFNULL(@input_date, CURRENT_DATE('{date_timezone}'));
            SET end_date= (SELECT DATE_SUB(input_date, INTERVAL 15 DAY));
            CALL `{user_scoped_segmentation_metrics_procedure_name}`input_date, end_date, users_added);"
          query_user_segmentation_dimensions: "
            DECLARE input_date DATE;
            DECLARE end_date DATE;
            DECLARE users_added INT64 DEFAULT NULL;
            SET input_date= IFNULL(@input_date, CURRENT_DATE('{date_timezone}'));
            SET end_date= (SELECT DATE_SUB(input_date, INTERVAL 15 DAY));
            CALL `{user_segmentation_dimensions_procedure_name}`(input_date, end_date, users_added);"
          query_user_session_event_aggregated_metrics: "
            DECLARE input_date DATE;
            DECLARE end_date DATE;
            DECLARE users_added INT64 DEFAULT NULL;
            SET input_date= IFNULL(@input_date, CURRENT_DATE('{date_timezone}'));
            SET end_date= (SELECT DATE_SUB(input_date, INTERVAL 30 DAY));
            CALL `{user_session_event_aggregated_metrics_procedure_name}`(input_date, end_date, users_added);"
          query_parameters:
            - { name: "input_date", type: "DATE", value: None } # If value is not defined then assume current_date()
          #INT64
          timeout: 600.0
        pipeline_parameters_substitutions: # Substitutions are applied to the parameters before compilation
          customer_lifetime_value_label_procedure_name: "${project_id}.feature_store.customer_lifetime_value_label"
          purchase_propensity_label_procedure_name: "${project_id}.feature_store.purchase_propensity_label"
          user_dimensions_procedure_name: "${project_id}.feature_store.user_dimensions"
          user_lifetime_dimensions_procedure_name: "${project_id}.feature_store.user_lifetime_dimensions"
          user_lookback_metrics_procedure_name: "${project_id}.feature_store.user_lookback_metrics"
          user_rolling_window_lifetime_metrics_procedure_name: "${project_id}.feature_store.user_rolling_window_lifetime_metrics"
          user_rolling_window_metrics_procedure_name: "${project_id}.feature_store.user_rolling_window_metrics"
          user_scoped_lifetime_metrics_procedure_name: "${project_id}.feature_store.user_scoped_lifetime_metrics"
          user_scoped_metrics_procedure_name: "${project_id}.feature_store.user_scoped_metrics"
          user_scoped_segmentation_metrics_procedure_name: "${project_id}.feature_store.user_scoped_segmentation_metrics"
          user_segmentation_dimensions_procedure_name: "${project_id}.feature_store.user_segmentation_dimensions"
          user_session_event_aggregated_metrics_procedure_name: "${project_id}.feature_store.user_session_event_aggregated_metrics"
          date_timezone: "UTC" # used when input_date is None and need to get current date.
        
    propensity:
      training:
        name: "propensity-training-pl"
        job_id_prefix: "propensity-training-pl-"
        experiment_name: "propensity-training-dev"
        type: "tabular-workflows"
        schedule:
          cron: "TZ=America/New_York */1 * * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project: "${project_id}"
          location: "us-central1"
          root_dir: "gs://${project_id}-pipelines/automl-tabular-training"
          transformations: "gs://${project_id}-pipelines/automl-tabular-training/transformations_config_{timestamp}.json"
          train_budget_milli_node_hours: 1000 # 1 hour
          run_evaluation: true
          run_distillation: false
          prediction_type: "classification"
          optimization_objective: "minimize-log-loss"
          target_column: "will_purchase"
          predefined_split_key: "data_split"
          data_source_csv_filenames: null
          training_fraction: 0.8
          validation_fraction: 0.1
          test_fraction: 0.1
          # data_source_bigquery_table_path: "bq://${project_id}.purchase_propensity.v_purchase_propensity_training_15_7"
          # data_source_bigquery_table_path: "bq://${project_id}.purchase_propensity.v_purchase_propensity_training_15_15"
          data_source_bigquery_table_path: "bq://${project_id}.purchase_propensity.v_purchase_propensity_training_30_15"
          dataflow_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"
          timestamp_split_key: null
          stratified_split_key: null
          weight_column: null
          additional_experiments: null
          export_additional_model_without_custom_ops: false
          study_spec_parameters_override:
            - parameter_id: "model_type"
              categorical_value_spec:
                values: 
                  - nn
                  - boosted_trees
        exclude_features:
          - processed_timestamp
          - data_split
          - feature_date
          - user_pseudo_id
          - user_id
          - will_purchase

        pipeline_parameters_substitutions: null 
      prediction:
        name: "propensity-prediction-pl"
        job_id_prefix: "propensity-prediction-pl-"
        experiment_name: "propensity-prediction-dev"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York */1 * * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "us-central1"
          model_display_name: "propensity-training-pl-model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          model_metric_name: "logLoss"
          model_metric_threshold: 0.9
          number_of_models_considered: 1
          bigquery_source: "${project_id}.purchase_propensity.v_purchase_propensity_inference_30_15_1"
          bigquery_destination_prefix: "${project_id}.purchase_propensity.purchase_propensity_inference_pl_"
          job_name_prefix: "propensity-prediction-pl-"
          machine_type: "n1-standard-4"
          max_replica_count: 10
          batch_size: 64
          accelerator_count: 0
          accelerator_type: "ACCELERATOR_TYPE_UNSPECIFIED" # ONE OF ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100, NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4
          generate_explanation: false
          threashold: 0.5
          positive_label: "1"
          pubsub_activation_topic: "activation-trigger"
          pubsub_activation_event_name: "propensity_prediction"
        pipeline_parameters_substitutions: null

    segmentation:
      training:
        name: "segmentation-training-pl"
        job_id_prefix: "segmentation-training-pl-"
        experiment_name: "segmentation-training-dev"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York */1 * * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "US"
          km_num_clusters: 4
          km_init_method: "KMEANS++"
          km_distance_type: "EUCLIDEAN"
          km_standardize_features: "TRUE"
          km_max_interations: 20
          km_early_stop: "TRUE"
          km_min_rel_progress: 0.01
          km_warm_start: "FALSE"
          model_bq_name: "${project_id}.km_us_test.customer_seg_model"
          training_data_bq_table: "${project_id}.ds.bank_marketing"
          timeout: 600
        pipeline_parameters_substitutions: null
      prediction:
        name: "segmentation-prediction-pl"
        job_id_prefix: "segmentation-prediction-pl-"
        experiment_name: "segmentation-prediction-dev"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York */1 * * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "US"
          model_dataset_id: "${project_id}.km_us_test" # to also include project.dataset
          model_name_bq_prefix: "customer_seg_model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          model_metric_name: "davies_bouldin_index" # one of davies_bouldin_index ,  mean_squared_distance
          model_metric_threshold: 10 
          number_of_models_considered: 2
          bigquery_source: "${project_id}.ds.bank_marketing"
          bigquery_destination_prefix: "${project_id}.ds.bank_seg_pred"
          #pubsub_activation_topic: "activation-trigger"
          #pubsub_activation_event_name: "segmentation"
        pipeline_parameters_substitutions: null

    clv:
      training:
        name: "clv-training-pl"
        job_id_prefix: "clv-training-pl-"
        experiment_name: "clv-training-dev"
        type: "tabular-workflows"
        schedule:
          cron: "TZ=America/New_York */1 * * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project: "${project_id}"
          location: "us-central1"
          root_dir: "gs://${project_id}-pipelines/clv-training"
          transformations: "gs://${project_id}-pipelines/clv-training/transformations_config_{timestamp}.json"
          train_budget_milli_node_hours: 1000 # 1 hour
          run_evaluation: true
          run_distillation: false
          prediction_type: "regression"
          target_column: "pltv_revenue_30_days"
          predefined_split_key: "data_split"
          training_fraction: null
          validation_fraction: null
          test_fraction: null
          data_source_csv_filenames: null
          optimization_objective: minimize-mae # minimize-mae | minimize-rmse | minimize-rmsle
          data_source_bigquery_table_path: "bq://${project_id}.customer_lifetime_value.v_customer_lifetime_value_training_180_30"
          dataflow_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"
          timestamp_split_key: null
          stratified_split_key: null
          weight_column: null
          additional_experiments: null
          export_additional_model_without_custom_ops: false
          study_spec_parameters_override:
          - parameter_id: "model_type"
            categorical_value_spec:
              values: 
                - nn
                - boosted_trees
        exclude_features:
          - processed_timestamp
          - data_split
          - feature_date
          - user_pseudo_id
          - user_id
        pipeline_parameters_substitutions: null 
      prediction:
        name: "clv-prediction-pl"
        job_id_prefix: "clv-prediction-pl-"
        experiment_name: "clv-prediction-dev"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York */1 * * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "us-central1"
          model_display_name: "training-pipeline-model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          model_metric_name: "logLoss"
          model_metric_threshold: 0.8
          number_of_models_considered: 1
          bigquery_source: "${project_id}.customer_lifetime_value.v_customer_lifetime_value_inference_180_30"
          bigquery_destination_prefix: "${project_id}.customer_lifetime_value.v_clv_inference_180_30"
          job_name_prefix: "bank_marketing"
          machine_type: "n1-standard-4"
          max_replica_count: 10
          batch_size: 64
          accelerator_count: 0
          accelerator_type: "ACCELERATOR_TYPE_UNSPECIFIED" # ONE OF ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100, NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4
          generate_explanation: false
          threashold: 0.5
          positive_label: "true"
          pubsub_activation_topic: "activation-trigger"
          pubsub_activation_event_name: "clv_prediction"
        pipeline_parameters_substitutions: null
        
    # segmentation_prediction_pipeline:
    #   name: "segmentation-prediction-pipeline"
    #   job_id_prefix: "seg-pred-pl-"
    #   experiment_name: "seg-pred-dev"
    #   type: "custom"
    #   schedule:
    #     cron: "TZ=America/New_York */1 * * * *"
    #     max_concurrent_run_count: 1
    #     start_time: null
    #     end_time: null
    #     state: PAUSED # possible states ACTIVE or PAUSED
    #   pipeline_parameters:
    #     project_id: "${project_id}"
    #     location: "US"
    #     model_dataset_id: "${project_id}.km_us_test" # to also include project.dataset
    #     model_name_bq_prefix: "customer_seg_model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
    #     model_metric_name: "davies_bouldin_index" # one of davies_bouldin_index ,  mean_squared_distance
    #     model_metric_threshold: 10 
    #     number_of_models_considered: 2
    #     bigquery_source: "${project_id}.ds.bank_marketing"
    #     bigquery_destination_prefix: "${project_id}.ds.bank_seg_pred"
    #     pubsub_activation_topic: "activation-trigger"
    #     pubsub_activation_event_name: "segmentation"
    #   pipeline_parameters_substitutions: null

bigquery:
  project_id: "${project_id}"
  region: "US"
  dataset:
    feature_store:
      project_id: "${project_id}" 
      name: "feature_store"
      location: "us"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Feature Store dataset for Marketing behavioural modeling"
      friendly_name: "Feature Store"
      max_time_travel_hours: 48
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    purchase_propensity:
      name: "purchase_propensity"
      region: "US"
      project_id: "${project_id}" 
      location: "us"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Purchase Propensity Use Case dataset for Marketing behavioural modeling"
      friendly_name: "Purchase Propensity Dataset"
      max_time_travel_hours: 48
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    customer_lifetime_value:
      project_id: "${project_id}" 
      name: "customer_lifetime_value"
      location: "us"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Customer Lifetime Value Use Case dataset for Marketing behavioural modeling"
      friendly_name: "Customer Lifetime Value Dataset"
      max_time_travel_hours: 48
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    audience_segmentation:
      project_id: "${project_id}" 
      name: "audience_segmentation"
      location: "us"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Audience Segmentation Use Case dataset for Marketing behavioural modeling"
      friendly_name: "Audience Segmentation Dataset"
      max_time_travel_hours: 48
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
  table:
    audience_segmentation_inference_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      table_name: "audience_segmentation_inference_preparation"
      region: "US"
      table_description: "Audience Segmentation Inference Preparation table to be used for Model Prediction"
    customer_lifetime_value_inference_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      table_name: "customer_lifetime_value_inference_preparation"
      region: "US"
      table_description: "Customer Lifetime Value Inference Preparation table to be used for Model Prediction"
    customer_lifetime_value_label:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      table_name: "customer_lifetime_value_label"
      region: "US"
      table_description: "Customer Lifetime Value Inference Preparation table to be used for Model Prediction"
    purchase_propensity_inference_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      table_name: "purchase_propensity_inference_preparation"
      region: "US"
      table_description: "Purchase Propensity Inference Preparation table to be used for Model Prediction"
    purchase_propensity_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "purchase_propensity_label"
      region: "US"
      table_description: "Purchase Propensity Label table to be used for Model Prediction"
    user_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_dimensions"
      region: "US"
      table_description: "User Dimensions table as part of the Feature Store"
    user_lifetime_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_lifetime_dimensions"
      region: "US"
      table_description: "User Lifetime Dimensions table as part of the Feature Store"
    user_lookback_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_lookback_metrics"
      region: "US"
      table_description: "User Lookback Metrics table as part of the Feature Store"
    user_rolling_window_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_rolling_window_lifetime_metrics"
      region: "US"
      table_description: "User Rolling Window Lifetime Metrics table as part of the Feature Store" 
    user_rolling_window_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_rolling_window_metrics"
      region: "US"
      table_description: "User Rolling Window Metrics table as part of the Feature Store"
    user_scoped_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_scoped_lifetime_metrics"
      region: "US"
      table_description: "User Scoped Lifetime Metrics table as part of the Feature Store"
    user_scoped_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_scoped_metrics"
      region: "US"
      table_description: "User Scoped Metrics table as part of the Feature Store"
    user_scoped_segmentation_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_scoped_segmentation_metrics"
      region: "US"
      table_description: "User Scoped Segmentation Metrics table as part of the Feature Store" 
    user_segmentation_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_segmentation_dimensions"
      region: "US"
      table_description: "User Segmentation Dimensions table as part of the Feature Store"
    user_session_event_aggregated_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_session_event_aggregated_metrics"
      region: "US"
      table_description: "User Session Event Aggregated Metrics table as part of the Feature Store" 
  query:
    audience_segmentation_query_template:
      none: none
    purchase_propensity_query_template:
      none: none
    cltv_query_template:
      none: none
    invoke_purchase_propensity_training_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      stored_procedure: "purchase_propensity_training_preparation"
      interval_max_date: 15
      interval_min_date: 30
      train_start_date: "2022-01-01"
      train_end_date: "2023-03-12"
      train_split_end_number: 5
      validation_split_end_number: 8
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    invoke_audience_segmentation_training_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      stored_procedure: "audience_segmentation_training_preparation"
      interval_max_date: 1
      interval_min_date: 15
      train_start_date: "2022-01-01"
      train_end_date: "2023-03-12"
      train_split_end_number: 5
      validation_split_end_number: 8
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    invoke_customer_lifetime_value_training_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      stored_procedure: "customer_lifetime_value_training_preparation"
      interval_max_date: 180
      interval_min_date: 180
      train_start_date: "2022-01-01"
      train_end_date: "2023-03-12"
      train_split_end_number: 5
      validation_split_end_number: 8
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}" 
    invoke_backfill_user_lifetime_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_lifetime_dimensions"
      interval_max_date: 180
      interval_min_date: 180
      interval_end_date: 180
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    invoke_backfill_user_scoped_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_scoped_lifetime_metrics"
      interval_max_date: 180
      interval_min_date: 180
      interval_end_date: 180
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}" 
    invoke_backfill_user_session_event_aggregated_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_session_event_aggregated_metrics"
      interval_max_date: 15
      interval_min_date: 30
      interval_end_date: 30
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}" 
    invoke_backfill_customer_lifetime_value_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "customer_lifetime_value_label"
      interval_max_date: 180
      interval_min_date: 180
      interval_end_date: 180
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    invoke_backfill_user_lookback_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_lookback_metrics"
      interval_min_date: 15
      interval_end_date: 15
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}" 
    invoke_backfill_user_rolling_window_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_rolling_window_lifetime_metrics"
      interval_max_date: 180
      interval_min_date: 180
      interval_end_date: 180
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}" 
    invoke_backfill_user_scoped_segmentation_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_scoped_segmentation_metrics"
      interval_min_date: 15
      interval_end_date: 15
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}" 
    invoke_backfill_user_segmentation_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_segmentation_dimensions"
      interval_min_date: 15
      interval_end_date: 15
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    invoke_backfill_purchase_propensity_label:
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "purchase_propensity_label"
      interval_max_date: 15
      interval_min_date: 30
      interval_end_date: 30
    invoke_backfill_user_dimensions:
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_dimensions"
      interval_max_date: 15
      interval_min_date: 30
      interval_end_date: 30
    invoke_backfill_user_rolling_window_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_rolling_window_metrics"
      interval_max_date: 15
      interval_min_date: 30
      interval_end_date: 30
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}" 
    invoke_backfill_user_scoped_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_scoped_metrics"
      interval_max_date: 15
      interval_min_date: 30
      interval_end_date: 30
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}" 
    invoke_customer_lifetime_value_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "customer_lifetime_value_label"
      interval_input_date: 180
    invoke_purchase_propensity_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "purchase_propensity_label"
      interval_input_date: 15
    invoke_user_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_dimensions"
      interval_end_date: 30
    invoke_user_lifetime_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_lifetime_dimensions"
      interval_end_date: 180 
    invoke_user_scoped_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_scoped_lifetime_metrics"
      interval_end_date: 180 
    invoke_user_session_event_aggregated_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_session_event_aggregated_metrics"
      interval_end_date: 30
    invoke_user_lookback_metrics: 
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_lookback_metrics"
      interval_end_date: 15
    invoke_user_rolling_window_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_rolling_window_lifetime_metrics"
      interval_end_date: 180
    invoke_user_scoped_segmentation_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_scoped_segmentation_metrics"
      interval_end_date: 15
    invoke_user_segmentation_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_segmentation_dimensions"
      interval_end_date: 15
    invoke_user_rolling_window_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_rolling_window_metrics"
      interval_end_date: 30
    invoke_user_scoped_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_scoped_metrics"
      interval_end_date: 30 
    invoke_purchase_propensity_inference_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      stored_procedure: "purchase_propensity_inference_preparation"
    invoke_customer_lifetime_value_inference_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      stored_procedure: "customer_lifetime_value_inference_preparation"
    invoke_audience_segmentation_inference_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      stored_procedure: "audience_segmentation_inference_preparation"
  procedure:
    audience_segmentation_training_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      name: "audience_segmentation_training_preparation"
      insert_table: "audience_segmentation_training_full_dataset"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
      expiration_duration_hours: 48
    customer_lifetime_value_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "customer_lifetime_value_label"
      insert_table: "customer_lifetime_value_label"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    customer_lifetime_value_training_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      name: "customer_lifetime_value_training_preparation"
      insert_table: "customer_lifetime_value_training_full_dataset"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
      expiration_duration_hours: 48
    purchase_propensity_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "purchase_propensity_label"
      insert_table: "purchase_propensity_label"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    purchase_propensity_training_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      name: "purchase_propensity_training_preparation"
      insert_table: "purchase_propensity_training_full_dataset"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
      expiration_duration_hours: 48
    user_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_dimensions"
      insert_table: "user_dimensions"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    user_lifetime_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_lifetime_dimensions"
      insert_table: "user_lifetime_dimensions"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    user_lookback_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_lookback_metrics"
      insert_table: "user_lookback_metrics"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    user_rolling_window_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_rolling_window_lifetime_metrics"
      insert_table: "user_rolling_window_lifetime_metrics"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    user_rolling_window_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_rolling_window_metrics"
      insert_table: "user_rolling_window_metrics"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    user_scoped_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_scoped_lifetime_metrics"
      insert_table: "user_scoped_lifetime_metrics"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    user_scoped_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_scoped_metrics"
      insert_table: "user_scoped_metrics"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    user_scoped_segmentation_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_scoped_segmentation_metrics"
      insert_table: "user_scoped_segmentation_metrics"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    user_segmentation_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_segmentation_dimensions"
      insert_table: "user_segmentation_dimensions"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    user_session_event_aggregated_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_session_event_aggregated_metrics"
      insert_table: "user_session_event_aggregated_metrics"
      mds_project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
    purchase_propensity_inference_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      name: "purchase_propensity_inference_preparation"
      feature_store_project_id: "${project_id}" 
      feature_store_dataset: "feature_store"
      insert_table: "purchase_propensity_inference_preparation"
      expiration_duration_hours: 48
    customer_lifetime_value_inference_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      name: "customer_lifetime_value_inference_preparation"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      insert_table: "customer_lifetime_value_inference_preparation"
      expiration_duration_hours: 48
    audience_segmentation_inference_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      name: "audience_segmentation_inference_preparation"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      insert_table: "audience_segmentation_inference_preparation"
      expiration_duration_hours: 48
      mds_dataset: "${mds_dataset}"
  models:
    create_purchase_propensity_bqml:
      project_id: "${project_id}"
      dataset: "model"
      name: "purchase_propensity_model_baseline"
      description: "BQML"
      model_type: "LOGISTIC_REG"
      data_split_column: "validation_row"
      label_column: "_1_15_day_will_purchase"
      model_registry: "vertex_ai"
      model_version_alias:
      - purchase_propensity
      - experimental
      training_project_id: "${project_id}"
      training_dataset_id: "feature_store"
      training_view: "purchase_propensity_training_dataset_30_15"
    create_customer_ltv_bqml:
      project_id: "${project_id}"
      dataset: "model"
      name: "customer_ltv_model_baseline"
      description: "BQML"
      model_type: "LINEAR_REG"
      data_split_column: "validation_row"
      label_column: "user_ltv_revenue"
      model_registry: "vertex_ai"
      model_version_alias:
      - customer_ltv
      - experimental
      training_project_id: "${project_id}"
      training_dataset_id: "feature_store"
      training_view: "customer_ltv_training_dataset_30_15"
    create_audience_segmentation_bqml:
      project_id: "${project_id}"
      dataset: "model"
      name: "audience_segmentation_model_baseline"
      description: "BQML"
      model_type: "KMEANS++"
      model_registry: "vertex_ai"
      model_version_alias:
      - customer_ltv
      - experimental
      training_project_id: "${project_id}"
      training_dataset_id: "feature_store"
      training_view: "audience_segmentation_training_dataset_30_15"

terraform:
  version: 1.3.4
  url: "https://releases.hashicorp.com/terraform/1.3.4/terraform_1.3.4_darwin_amd64.zip" 
