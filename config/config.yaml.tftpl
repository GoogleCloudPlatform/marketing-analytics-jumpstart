# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This *.tftpl file is a Terraform template file. It is used to create or update 
# resources in a Google Cloud project. A Terraform template file contains the 
# configuration parameters for the resources that you want to create or update. 
# It also contains the logic for creating or updating the resources. Terraform 
# template files are typically used to create or update resources that are 
# complex or that require a lot of configuration.
#
# The config.yaml.tftpl file is a YAML file that contains all the configuration 
# parameters for the marketing analytics jumpstart solution. 

# It contains the following sections:
#
# google_cloud_project: This section contains the Google Cloud project ID and project number.
# cloud_build: This section contains the configuration parameters for the Cloud Build pipeline.
# container: This section contains the configuration parameters for the container images.
# artifact_registry: This section contains the configuration parameters for the Artifact Registry repository.
# dataflow: This section contains the configuration parameters for the Dataflow pipeline.
# vertex_ai: This section contains the configuration parameters for the Vertex AI pipeline.
# bigquery: This section contains the configuration parameters for the BigQuery artifacts.

# This block contains general configuration parameters for the Google Cloud Project.
google_cloud_project:
  project_id: "${project_id}" # project_id terraform variable hidrated by terraform.
  project_name: "${project_name}" # project name terraform variable hidrated by terraform.
  project_number: "${project_number}" # project number terraform variable hidrated by terraform.
  region: "${cloud_region}" # region terraform variable hidrated by terraform.

# This block contains configuration parameters for the Cloud Build steps.
# The CI/CD pipelines are going to use Cloud Build to validate and test the solution at any time the
# developer wants.
# The Cloud Build pipeline will be created via a Terraform resource.
# This capability is still under construction, it is not ready to use.
cloud_build:
  project_id: "${project_id}"
  region: "${cloud_region}"
  github:
    owner: "${pipelines_github_owner}"
    repo_name: "${pipelines_github_repo}"
    trigger_branch: "dev"
  build_file: "cloudbuild/pipelines.yaml"
  _REPOSITORY_GCP_PROJECT: "${project_id}"
  _REPOSITORY_NAME: "github_${pipelines_github_owner}_${pipelines_github_repo}"
  _REPOSITORY_BRANCH: "main"
  _GCR_HOSTNAME: "${cloud_region}-docker.pkg.dev"
  _BUILD_REGION: "${cloud_region}"

# This block contains configuration parameters for the container images.
# The CI/CD pipelines are going to use the container parameters to auxiliate the Docker containers
# creation.
# The container images will be created via a Terraform resource.
# This capability is still under construction, it is not ready to use.
container:
  builder:
    # This is the base image used to run linting, formatting and unit tests of the python code.
    base:
      from_image: "python:3.7-alpine3.7"
      base_image_name: "base-builder"
      base_image_prefix: "maj"
    # This is the zetasql formatter image used to format and validate the SQL queries.
    zetasql:
      from_image: "wbsouza/zetasql-formatter:latest"
      base_image_name: "zetasql-formatter"
      base_image_prefix: "maj"
  container_registry_hostname: "${cloud_region}-docker.pkg.dev"
  container_registry_region: "${cloud_region}"

# This block contains configuration parameters for the Artifact Registry repositories.
# The Pipelines terraform module uses Artifact Registry to store the pipelines YAML configuration files
# and the docker images.
# Two repositories are created: one for the pipelines and one for the docker images.
# The pipelines repository is used to store the pipelines YAML configuration files to be compiled 
# and uploaded via a Terraform resource.
# The docker repository is used to store the Docker image built via a Terraform resource. 
# The only image being built right now is the pipeline components container image.
artifact_registry:
  pipelines_repo:
    name: "pipelines-repo"
    region: "${cloud_region}"
    project_id: "${project_id}"
  pipelines_docker_repo:
    name: "pipelines-docker-repo"
    region: "${cloud_region}"
    project_id: "${project_id}"

# This block contains configuration parameters for the Dataflow jobs and templates.
# The Activation Application terraform module uses Dataflow templates to create the Dataflow job.
# The Dataflow job is responsible for sending the model predictions to the GA4 / GAds platforms via
# Measurement Protocol API.
dataflow:
  # The `worker_service_account_id` is the service account ID used by the dataflow workers. 
  worker_service_account_id: "df-worker"
  # The `worker_service_account` is the service account used by the dataflow workers. 
  # The service account is created via a Terraform resource.
  worker_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"

# This block contains configuration parameters for the Vertex AI pipeline components.
# The Pipelines terraform module uses Vertex AI pipeline components to create the Vertex AI pipeline.
# The Vertex AI pipeline uses a Docker image at every step which is customized for this solution.
# Navigate to Vertex AI -> Training -> Custom Job to see the pipeline components executions.
vertex_ai:
  # The components block contains configuration parameters for the Vertex AI pipeline components.
  components:
    # The `base_image_name` is the name of the Docker container image used by the pipeline components.
    # The base image is created via a Terraform resource.
    # The Dockerfile recipe can be found at `pipelines/base_component_image/Dockerfile`.
    # Any python library dependencies should be installed in the `pyproject.toml` file.
    # The `pyproject.toml` file is a configuration file used by packaging tools, as well as other 
    # tools such as linters, type checkers, etc.
    # The `pyproject.toml` file can be found at `pipelines/base_component_image/pyproject.toml`.
    base_image_name: "ma-components"
    base_image_tag: "dev"

  # This pipelines block contains configuration parameters for the Vertex AI pipelines.
  # The current pipelines are:
  # - feature-creation-auto-audience-segmentation
  # - feature-creation-audience-segmentation
  # - feature-creation-purchase-propensity
  # - feature-creation-customer-ltv
  # - propensity
  #   - training
  #   - prediction
  # - segmentation
  #   - training
  #   - prediction
  # - auto_segmentation
  #   - training
  #   - prediction
  # - propensity_clv
  #   - training
  # - clv
  #   - training
  #   - prediction
  pipelines: 
    project_id: "${project_id}"
    service_account_id: "vertex-pipelines-sa"
    service_account: "vertex-pipelines-sa@${project_id}.iam.gserviceaccount.com"
    region: "${cloud_region}"
    bucket_name: "${project_id}-pipelines"
    model_bucket_name: "${project_id}-custom-models"
    root_path: "gs://${project_id}-pipelines/pipelines/"

    # This pipeline contains the configuration parameters for the feature creation pipeline for the auto audience segmentation model.
    # This block defines the pipeline parameters that are going to be used for three tasks: compilation, upload and scheduling.
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the queries and procedures SQL parameters in this file under the `bigquery` section, following YAML format
    ## 3. Create the queries and procedures SQL files under sql/ folder
    ## 4. Create the terraform resources uin terraform/feature-store/bigquery-procedures.tf
    ## 5. Create the terraform resources to compile and schedule the pipeline in terraform/pipelines/pipelines.tf
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `python/pipelines/compiler.py` and `python/pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `python/pipelines/scheduler.py`.
    ## 8. Create the pipeline python function in python/pipelines/feature_engineering_pipelines.py
    ## 9. Run terraform apply
    feature-creation-auto-audience-segmentation:
      execution:
        # The `name` parameter is the name of the pipeline that will appear in the Vertex AI pipeline UI.
        name: "feature-creation-auto-audience-segmentation"
        # The `job_id_prefix` is the prefix of the Vertex AI Custom Job that will be used at the execution of each individual component step.
        job_id_prefix: "feature-creation-auto-audience-segmentation-"
        # The `experiment_name` is the name of the experiment that will appear in the Vertex AI Experiments UI.
        experiment_name: "feature-creation-auto-audience-segmentation"
        # The `type` defines whether the pipeline is going to be a `tabular-workflows` or a `custom` pipeline.
        # `type` must be "custom", when we're building Python and/or SQL based pipelines for feature engineering purposes.
        type: "custom"
        # The `schedule` defines the schedule values of the pipeline.
        # This solution uses the Vertex AI Pipeline Scheduler.
        # More information can be found at https://cloud.google.com/vertex-ai/docs/pipelines/scheduler.
        schedule:
          # The `cron` is the cron schedule. Make sure you review the TZ=America/New_York timezone.
          # More information can be found at https://cloud.google.com/scheduler/docs/configuring/cron-job-schedules.
          cron: "TZ=America/New_York 0 1 * * *"
          # The `max_concurrent_run_count` defines the maximum number of concurrent pipeline runs.
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          # The `state` defines the state of the pipeline.
          # In case you don't want to schedule the pipeline, set the state to `PAUSED`.
          state: PAUSED # possible states ACTIVE or PAUSED
        # The `pipeline_parameters` defines the parameters that are going to be used to compile the pipeline.
        # Those values may difer depending on the pipeline type and the pipeline steps being used.
        # Make sure you review the python function the defines the pipeline.
        # The pipeline definition function can be found in `python/pipelines/feature_engineering_pipelines.py` 
        # or other files ending with `python/pipelines/*_pipeline.py`.
        # Auto Audience Segmentation involved the dynamic identification of the most visited pages in the website by using the regular expression `reg_expression` and frequency percentual `perc_keep`.
        # This process takes into consideration a date interval defined by the parameters `date_start` and `date_end`.
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          dataset: "auto_audience_segmentation"
          feature_table: "page_path_cumulative_traffic"
          date_start: "2023-01-01"
          date_end: "2024-04-01"
          mds_project_id: "${mds_project_id}"
          mds_dataset: "${mds_dataset}"
          stored_procedure_name: "auto_audience_segmentation_dataset_preparation"
          full_dataset_table: "auto_audience_segmentation_full_dataset"
          #training_table: "auto_audience_segmentation_training_15"
          #inference_table: "auto_audience_segmentation_inference_15"
          # Change the regular expression to match the URL of your website to be categorized.
          #reg_expression: "^${website_url}([-a-zA-Z0-9@:%_+.~#?//=]*)(?:/)$"
          reg_expression: "^${website_url}([-a-zA-Z0-9@:%_+.~#?//=]*)$"
          # Increase the percentual `perc_keep` to include more pages in your segmentation, decrease to exclude pages in your segmentation
          perc_keep: 35
          query_auto_audience_segmentation_inference_preparation: "
            CALL `{auto_audience_segmentation_inference_preparation_procedure_name}`();"
          query_auto_audience_segmentation_training_preparation: "
            CALL `{auto_audience_segmentation_training_preparation_procedure_name}`();"
          # The `timeout` parameter defines the timeout of the pipeline in seconds.
          # The default value is 3600 seconds (1 hour).
          timeout: 3600.0
        # The `pipeline_parameters_substitutions` defines the substitutions that are going to be applied to the pipeline parameters before compilation.
        # Check the parameter values above to see if they are used. 
        # They typically follow this format {parameter_subsititution_key}.
        # To apply a substitution, make sure you define the pair: {parameter_subsititution_key}: {parameter_subsititution_value}.
        pipeline_parameters_substitutions:
          date_timezone: "UTC" # used when input_date is None and need to get current date.
          auto_audience_segmentation_inference_preparation_procedure_name: "${project_id}.auto_audience_segmentation.invoke_auto_audience_segmentation_inference_preparation"
          auto_audience_segmentation_training_preparation_procedure_name: "${project_id}.auto_audience_segmentation.invoke_auto_audience_segmentation_training_preparation"
    
    # This pipeline contains the configuration parameters for the feature creation pipeline for the audience segmentation model.
    # This block defines the pipeline parameters that are going to be used for three tasks: compilation, upload and scheduling.
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the queries and procedures SQL parameters in this file under the `bigquery` section, following YAML format
    ## 3. Create the queries and procedures SQL files under sql/ folder
    ## 4. Create the terraform resources uin terraform/feature-store/bigquery-procedures.tf
    ## 5. Create the terraform resources to compile and schedule the pipeline in terraform/pipelines/pipelines.tf
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `python/pipelines/compiler.py` and `python/pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `python/pipelines/scheduler.py`.
    ## 8. Create the pipeline python function in python/pipelines/feature_engineering_pipelines.py
    ## 9. Run terraform apply
    feature-creation-audience-segmentation:
      execution:
        # The `name` parameter is the name of the pipeline that will appear in the Vertex AI pipeline UI.
        name: "feature-creation-audience-segmentation"
        # The `job_id_prefix` is the prefix of the Vertex AI Custom Job that will be used at the execution of each individual component step.
        job_id_prefix: "feature-creation-audience-segmentation-"
        # The `experiment_name` is the name of the experiment that will appear in the Vertex AI Experiments UI.
        experiment_name: "feature-creation-audience-segmentation"
        # The `type` defines whether the pipeline is going to be a `tabular-workflows` or a `custom` pipeline.
        # `type` must be "custom", when we're building Python and/or SQL based pipelines for feature engineering purposes.
        type: "custom"
        # The `schedule` defines the schedule values of the pipeline.
        # This solution uses the Vertex AI Pipeline Scheduler.
        # More information can be found at https://cloud.google.com/vertex-ai/docs/pipelines/scheduler.
        schedule:
          # The `cron` is the cron schedule. Make sure you review the TZ=America/New_York timezone.
          # More information can be found at https://cloud.google.com/scheduler/docs/configuring/cron-job-schedules.
          cron: "TZ=America/New_York 0 1 * * *"
          # The `max_concurrent_run_count` defines the maximum number of concurrent pipeline runs.
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          # The `state` defines the state of the pipeline.
          # In case you don't want to schedule the pipeline, set the state to `PAUSED`.
          state: PAUSED # possible states ACTIVE or PAUSED
        # The `pipeline_parameters` defines the parameters that are going to be used to compile the pipeline.
        # Those values may difer depending on the pipeline type and the pipeline steps being used.
        # Make sure you review the python function the defines the pipeline.
        # The pipeline definition function can be found in `python/pipelines/feature_engineering_pipelines.py` 
        # or other files ending with `python/pipelines/*_pipeline.py`.
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          query_user_segmentation_dimensions: "
            CALL `{user_segmentation_dimensions_procedure_name}`();"
          query_user_lookback_metrics: "
            CALL `{user_lookback_metrics_procedure_name}`();"
          query_user_scoped_segmentation_metrics: "
            CALL `{user_scoped_segmentation_metrics_procedure_name}`();"
          query_audience_segmentation_inference_preparation: "
            CALL `{audience_segmentation_inference_preparation_procedure_name}`();"
          query_audience_segmentation_training_preparation: "
            CALL `{audience_segmentation_training_preparation_procedure_name}`();"
          # The `timeout` parameter defines the timeout of the pipeline in seconds.
          # The default value is 3600 seconds (1 hour).
          timeout: 3600.0
        # The `pipeline_parameters_substitutions` defines the substitutions that are going to be applied to the pipeline parameters before compilation.
        # Check the parameter values above to see if they are used. 
        # They typically follow this format {parameter_subsititution_key}.
        # To apply a substitution, make sure you define the pair: {parameter_subsititution_key}: {parameter_subsititution_value}.
        pipeline_parameters_substitutions: # Substitutions are applied to the parameters before compilation
          user_segmentation_dimensions_procedure_name: "${project_id}.feature_store.invoke_user_segmentation_dimensions"
          user_lookback_metrics_procedure_name: "${project_id}.feature_store.invoke_user_lookback_metrics"
          user_scoped_segmentation_metrics_procedure_name: "${project_id}.feature_store.invoke_user_scoped_segmentation_metrics"
          date_timezone: "UTC" # used when input_date is None and need to get current date.
          audience_segmentation_inference_preparation_procedure_name: "${project_id}.audience_segmentation.invoke_audience_segmentation_inference_preparation"
          audience_segmentation_training_preparation_procedure_name: "${project_id}.audience_segmentation.invoke_audience_segmentation_training_preparation"

    # This pipeline contains the configuration parameters for the feature creation pipeline for the purchase propensity model.
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the queries and procedures SQL parameters in this file under the `bigquery` section, following YAML format
    ## 3. Create the queries and procedures SQL files under sql/ folder
    ## 4. Create the terraform resources uin terraform/feature-store/bigquery-procedures.tf
    ## 5. Create the terraform resources to compile and schedule the pipeline in terraform/pipelines/pipelines.tf
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `python/pipelines/compiler.py` and `python/pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `python/pipelines/scheduler.py`.
    ## 8. Create the pipeline python function in python/pipelines/feature_engineering_pipelines.py
    ## 9. Run terraform apply
    feature-creation-purchase-propensity: 
      execution:
        name: "feature-creation-purchase-propensity"
        job_id_prefix: "feature-creation-purchase-propensity-"
        experiment_name: "feature-creation-purchase-propensity"
        # `type` must be "custom", when we're building Python and/or SQL based pipelines for feature engineering purposes.
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 1 * * *"
          # Define the maximum number of concurrent pipeline runs.
          # The default value is 1.
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          # The query_purchase_propensity_label defines the procedure that will be used to invoke the creation of the purchase propensity label feature table.
          query_purchase_propensity_label: "
            CALL `{purchase_propensity_label_procedure_name}`();"
          # The query_user_dimensions defines the procedure that will be used to invoke the creation of the user dimensions feature table.
          query_user_dimensions: "
            CALL `{user_dimensions_procedure_name}`();"
          # The query_user_rolling_window_metrics defines the procedure that will be used to invoke the creation of the user rolling window metrics feature table.
          query_user_rolling_window_metrics: "
            CALL `{user_rolling_window_metrics_procedure_name}`();"
          # The query_user_scoped_metrics defines the procedure that will be used to invoke the creation of the user scoped metrics feature table.
          query_user_scoped_metrics: "
            CALL `{user_scoped_metrics_procedure_name}`();"
          # The query_user_session_event_aggregated_metrics defines the procedure that will be used to invoke the creation of the user session and events aggregated metrics feature table.
          query_user_session_event_aggregated_metrics: "
            CALL `{user_session_event_aggregated_metrics_procedure_name}`();"
          # The query_purchase_propensity_inference_preparation define the procedure that will be used to invoke the creation of the purchase propensity inference preparation table.
          query_purchase_propensity_inference_preparation: "
            CALL `{purchase_propensity_inference_preparation_procedure_name}`();"
          # The query_purchase_propensity_training_preparation define the procedure that will be used to invoke the creation of the purchase propensity training preparation table.
          query_purchase_propensity_training_preparation: "
            CALL `{purchase_propensity_training_preparation_procedure_name}`();"
          timeout: 3600.0
        pipeline_parameters_substitutions: # Substitutions are applied to the parameters before compilation
          purchase_propensity_label_procedure_name: "${project_id}.feature_store.invoke_purchase_propensity_label"
          user_dimensions_procedure_name: "${project_id}.feature_store.invoke_user_dimensions"
          user_rolling_window_metrics_procedure_name: "${project_id}.feature_store.invoke_user_rolling_window_metrics"
          user_scoped_metrics_procedure_name: "${project_id}.feature_store.invoke_user_scoped_metrics"
          user_session_event_aggregated_metrics_procedure_name: "${project_id}.feature_store.invoke_user_session_event_aggregated_metrics"
          date_timezone: "UTC" # used when input_date is None and need to get current date.
          purchase_propensity_inference_preparation_procedure_name: "${project_id}.purchase_propensity.invoke_purchase_propensity_inference_preparation"
          purchase_propensity_training_preparation_procedure_name: "${project_id}.purchase_propensity.invoke_purchase_propensity_training_preparation"

    # This pipeline contains the configuration parameters for the feature creation pipeline for the customer lifetime value model.
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the queries and procedures SQL parameters in this file under the `bigquery` section, following YAML format
    ## 3. Create the queries and procedures SQL files under sql/ folder
    ## 4. Create the terraform resources uin terraform/feature-store/bigquery-procedures.tf
    ## 5. Create the terraform resources to compile and schedule the pipeline in terraform/pipelines/pipelines.tf
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `python/pipelines/compiler.py` and `python/pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `python/pipelines/scheduler.py`.
    ## 8. Create the pipeline python function in python/pipelines/feature_engineering_pipelines.py
    ## 9. Run terraform apply
    feature-creation-customer-ltv:
      execution:
        name: "feature-creation-customer-ltv"
        job_id_prefix: "feature-creation-customer-ltv-"
        experiment_name: "feature-creation-customer-ltv"
        # `type` must be "custom", when we're building Python and/or SQL based pipelines for feature engineering purposes.
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 1 * * *"
          # Define the maximum number of concurrent pipeline runs.
          # The default value is 1.
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          # The query_customer_lifetime_value_label defines the procedure that will be used to invoke the creation of the customer lifetime value label feature table.
          query_customer_lifetime_value_label: "
            CALL `{customer_lifetime_value_label_procedure_name}`();"
          # The query_user_lifetime_dimensions defines the procedure that will be used to invoke the creation of the user lifetime dimensions feature table.
          query_user_lifetime_dimensions: "
            CALL `{user_lifetime_dimensions_procedure_name}`();"
          # The query_user_rolling_window_lifetime_metrics defines the procedure that will be used to invoke the creation of the user rolling window lifetime metrics feature table.
          query_user_rolling_window_lifetime_metrics: "
            CALL `{user_rolling_window_lifetime_metrics_procedure_name}`();"
          # The query_user_scoped_lifetime_metrics defines the procedure that will be used to invoke the creation of the user scoped lifetime metrics feature table.
          query_user_scoped_lifetime_metrics: "
            CALL `{user_scoped_lifetime_metrics_procedure_name}`();"
          # The query_customer_lifetime_value_inference_preparation defines the procedure that will be used to invoke the creation of the customer lifetime value inference preparation table.
          query_customer_lifetime_value_inference_preparation: "
            CALL `{customer_lifetime_value_inference_preparation_procedure_name}`();"
          # The query_customer_lifetime_value_training_preparation defines the procedure that will be used to invoke the creation of the customer lifetime value training preparation table.
          query_customer_lifetime_value_training_preparation: "
            CALL `{customer_lifetime_value_training_preparation_procedure_name}`();"
          timeout: 3600.0
        pipeline_parameters_substitutions: # Substitutions are applied to the parameters before compilation
          customer_lifetime_value_label_procedure_name: "${project_id}.feature_store.invoke_customer_lifetime_value_label"
          user_lifetime_dimensions_procedure_name: "${project_id}.feature_store.invoke_user_lifetime_dimensions"
          user_rolling_window_lifetime_metrics_procedure_name: "${project_id}.feature_store.invoke_user_rolling_window_lifetime_metrics"
          user_scoped_lifetime_metrics_procedure_name: "${project_id}.feature_store.invoke_user_scoped_lifetime_metrics"
          date_timezone: "UTC" # used when input_date is None and need to get current date.
          customer_lifetime_value_inference_preparation_procedure_name: "${project_id}.customer_lifetime_value.invoke_customer_lifetime_value_inference_preparation"
          customer_lifetime_value_training_preparation_procedure_name: "${project_id}.customer_lifetime_value.invoke_customer_lifetime_value_training_preparation"

    # This pipeline contains the configuration parameters for the feature creation pipeline for the aggregated value based bidding model.
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the queries and procedures SQL parameters in this file under the `bigquery` section, following YAML format
    ## 3. Create the queries and procedures SQL files under sql/ folder
    ## 4. Create the terraform resources uin terraform/feature-store/bigquery-procedures.tf
    ## 5. Create the terraform resources to compile and schedule the pipeline in terraform/pipelines/pipelines.tf
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `python/pipelines/compiler.py` and `python/pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `python/pipelines/scheduler.py`.
    ## 8. Create the pipeline python function in python/pipelines/feature_engineering_pipelines.py
    ## 9. Run terraform apply
    feature-creation-aggregated-value-based-bidding:
      execution:
        name: "feature-creation-aggregated-value-based-bidding"
        job_id_prefix: "feature-creation-aggregated-value-based-bidding-"
        experiment_name: "feature-creation-aggregated-value-based-bidding"
        # `type` must be "custom", when we're building Python and/or SQL based pipelines for feature engineering purposes.
        type: "custom"
        schedule:
          # The cron string is
          cron: "TZ=America/New_York 0 1 * * *"
          # Define the maximum concurrent run count of the pipeline.
          # The default value is 1.
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          # The query_aggregated_value_based_bidding_training_preparation defines the procedure that will be used to invoke the creation of the aggregated value based bidding training preparation table.
          query_aggregated_value_based_bidding_training_preparation: "
            CALL `{aggregated_value_based_bidding_training_preparation_procedure_name}`();"
          # The query_aggregated_value_based_bidding_explanation_preparation defines the procedure that will be used to invoke the creation of the aggregated value based bidding explanation preparation table.
          query_aggregated_value_based_bidding_explanation_preparation: "
            CALL `{aggregated_value_based_bidding_explanation_preparation_procedure_name}`();"
          timeout: 3600
        pipeline_parameters_substitutions:
          aggregated_value_based_bidding_training_preparation_procedure_name: "${project_id}.aggregated_vbb.invoke_aggregated_value_based_bidding_training_preparation"
          aggregated_value_based_bidding_explanation_preparation_procedure_name: "${project_id}.aggregated_vbb.invoke_aggregated_value_based_bidding_explanation_preparation"
    
    # This pipeline contains the configuration parameters for the value based bidding training and inference pipelines.
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the bigquery or Vertex AI KFP components to be used by your pipeline in `python/pipelines` section, if applicable.
    ## 3. Define or reuse the pipeline definition method to be used to compile the pipeline into a YAML file in `python/pipelines` section.
    ## 5. Create the terraform resources to compile, upload and schedule the pipeline in `terraform/pipelines/pipelines.tf`
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `python/pipelines/compiler.py` and `python/pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `python/pipelines/scheduler.py`.
    ## 9. Run terraform apply
    ## Note: For `type` = "tabular workflows", the pre-compiled YAML file `automl_tabular_pl_v4.yaml` is recompiled by parsing the `pipeline_parameters` below as default values
    ## to the the new YAML file. The recompiled YAML will be uploaded and scheduled in Vertex AI Pipelines.
    value_based_bidding:
      training:
        name: "value-based-bidding-training-pl"
        job_id_prefix: "value-based-bidding-training-pl-"
        experiment_name: "value-based-bidding-training"
        # `type` can be "custom" or "tabular-workflows". 
        #  For using Vertex AI Tabular Workflow use the later, for all other modeling approaches use "custom" (i.e. BQML, Scikit-learn).
        type: "tabular-workflows"
        schedule:
          # define the schedule for the pipeline
          cron: "TZ=America/New_York 0 1 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        # These are pipeline parameters that will be passed to the pipeline to be recompiled
        pipeline_parameters:
          project: "${project_id}"
          location: "${cloud_region}"
          root_dir: "gs://${project_id}-pipelines/value-based-bidding"
          transformations: "gs://${project_id}-pipelines/value-based-bidding/transformations_config_{timestamp}.json"
          train_budget_milli_node_hours: 1000 # 1 hour
          run_evaluation: true
          run_distillation: false
          # Use `prediction_type` to "regression" for training models that predict a numerical value.  For classification models, use "classification" and you will
          # also get the probability likelihood for that class.
          prediction_type: "regression"
          # The optimization objectives may change depending on the `prediction_type`.
          # For binary classification, use "maximize-au-roc", "minimize-log-loss", "maximize-au-prc", "maximize-precision-at-recall" or "maximize-recall-at-precision". 
          # For multi class classification, use "minimize-log-loss". 
          # For regression, use "minimize-rmse", "minimize-mae", or "minimize-rmsle".
          optimization_objective: "minimize-mae" #minimize-rmse
          target_column: "Purchase_Product"
          predefined_split_key: "data_split"
          data_source_csv_filenames: null
          training_fraction: null
          validation_fraction: null
          test_fraction: null
          # This is the training dataset provided during the training routine.
          # The schema in this table or view must match the schema in the json files.
          # Take into consideration the `excluded_features` list below. They won't be used for training.
          data_source_bigquery_table_path: "bq://${project_id}.aggregated_vbb.aggregated_value_based_bidding_training_full_dataset"
          data_source_bigquery_table_schema: "../sql/schema/table/value_based_bidding_training_preparation.json"
          dataflow_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"
          timestamp_split_key: null
          stratified_split_key: null
          weight_column: null
          additional_experiments: null
          export_additional_model_without_custom_ops: false
          # Override the study spec parameters in case you want to restrict hyperparameter search space. Including `model_type`.
          # In this case, for Value Based Bidding, we're looking for a perfect fit using a tree based model.
          study_spec_parameters_override:
            - parameter_id: "model_type"
              categorical_value_spec:
                values: 
                  - boosted_trees
        # Features to be excluded from the training dataset.
        exclude_features:
          - Purchase_Product
          - Dt
          - data_split
        pipeline_parameters_substitutions: null
      explanation:
        name: "value-based-bidding-explanation-pl"
        job_id_prefix: "value-based-bidding-explanation-pl-"
        experiment_name: "value-based-bidding-explanation"
        # `type` can be "custom" or "tabular-workflows". 
        #  For using Vertex AI Tabular Workflow use the later, for all other modeling approaches use "custom" (i.e. BQML, Scikit-learn).
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 5 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project: "${project_id}"
          location: "${cloud_region}"
          data_location: "${location}"
          model_display_name: "value-based-bidding-training-pl-model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          model_metric_name: "meanAbsoluteError" #'rootMeanSquaredError', 'meanAbsoluteError', 'meanAbsolutePercentageError', 'rSquared', 'rootMeanSquaredLogError'
          # The `model_metric_threshold` parameter defines what is the maximum acceptable value for the `model_metric_name` so that the model can be selected.
          # If the actual models metrics values are higher than this limit, no models will be selected and the pipeline is going to fail.
          model_metric_threshold: 400
          number_of_models_considered: 1
          bigquery_destination_prefix: "${project_id}.aggregated_vbb.vbb_weights"
          # These are parameter to inovoke the aggregations of all daily predictions into a single table.
          aggregated_predictions_dataset_location: "${location}"
          query_aggregate_last_day_predictions: "CALL `${project_id}.aggregated_predictions.aggregate_last_day_predictions`();"
        pipeline_parameters_substitutions: null

    # This pipeline contains the configuration parameters for the propensity training and inference pipelines for the purchase propensity model.
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the bigquery or Vertex AI KFP components to be used by your pipeline in `python/pipelines` section, if applicable.
    ## 3. Define or reuse the pipeline definition method to be used to compile the pipeline into a YAML file in `python/pipelines` section.
    ## 5. Create the terraform resources to compile, upload and schedule the pipeline in `terraform/pipelines/pipelines.tf`
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `python/pipelines/compiler.py` and `python/pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `python/pipelines/scheduler.py`.
    ## 9. Run terraform apply
    ## Note: For `type` = "tabular workflows", the pre-compiled YAML file `automl_tabular_pl_v4.yaml` is recompiled by parsing the `pipeline_parameters` below as default values
    ## to the the new YAML file. The recompiled YAML will be uploaded and scheduled in Vertex AI Pipelines.
    propensity:
      training:
        name: "propensity-training-pl"
        job_id_prefix: "propensity-training-pl-"
        experiment_name: "propensity-training"
        # `type` can be "custom" or "tabular-workflows". 
        #  For using Vertex AI Tabular Workflow use the later, for all other modeling approaches use "custom" (i.e. BQML, Scikit-learn).
        type: "tabular-workflows"
        schedule:
          cron: "TZ=America/New_York 0 8 * * SAT"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        # These are pipeline parameters that will be passed to the pipeline to be recompiled
        pipeline_parameters:
          project: "${project_id}"
          location: "${cloud_region}"
          root_dir: "gs://${project_id}-pipelines/propensity-training"
          transformations: "gs://${project_id}-pipelines/propensity-training/transformations_config_{timestamp}.json"
          # These are specific data types transformations that will be applied to the dataset.
          custom_transformations: "pipelines/transformations-purchase-propensity.json"
          train_budget_milli_node_hours: 100 # 1000 = 1 hour 
          # Set these to apply feature selection tuning.
          max_selected_features: 20
          apply_feature_selection_tuning: true
          run_evaluation: true
          run_distillation: false
          # Use `prediction_type` to "regression" for training models that predict a numerical value.  For classification models, use "classification" and you will
          # also get the probability likelihood for that class.
          prediction_type: "classification"
          # The optimization objectives may change depending on the `prediction_type`.
          # For binary classification, use "maximize-au-roc", "minimize-log-loss", "maximize-au-prc", "maximize-precision-at-recall" or "maximize-recall-at-precision". 
          # For multi class classification, use "minimize-log-loss". 
          # For regression, use "minimize-rmse", "minimize-mae", or "minimize-rmsle".
          optimization_objective: "maximize-au-roc" # maximize-precision-at-recall, maximize-au-prc, maximize-au-roc, minimize-log-loss, maximize-recall-at-precision
          #Don't use when parameter `optimization_objective` is not `maximize-precision-at-recall` or `maximize-recall-at-precision`
          #optimization_objective_recall_value: 0.72
          #optimization_objective_precision_value: 0.72
          target_column: "will_purchase"
          predefined_split_key: "data_split"
          data_source_csv_filenames: null
          training_fraction: null
          validation_fraction: null
          test_fraction: null
          # This is the training dataset provided during the training routine.
          # The schema in this table or view must match the schema in the json files.
          # Take into consideration the `excluded_features` list below. They won't be used for training.
          data_source_bigquery_table_path: "bq://${project_id}.purchase_propensity.v_purchase_propensity_training_30_15_last_window"
          data_source_bigquery_table_schema: "../sql/schema/table/purchase_propensity_training_preparation.json"
          dataflow_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"
          timestamp_split_key: null
          stratified_split_key: null
          weight_column: null
          additional_experiments: null
          export_additional_model_without_custom_ops: false
          # Override the study spec parameters in case you want to restrict hyperparameter search space. Including `model_type`.
          # In this case, for Value Based Bidding, we're looking for a perfect fit using a tree based model.
          # Don't use when parameter `apply_feature_selection_tuning` is `true`
          #study_spec_parameters_override:
          #  - parameter_id: "model_type"
          #    categorical_value_spec:
          #      values: 
          #        - nn
          #        - boosted_trees
          #  - parameter_id: "feature_selection_rate"
          #    double_value_spec:
          #      min_value: 0.5
          #      max_value: 1.0
          #    scale_type: UNIT_LINEAR_SCALE
        # Features to be excluded from the training dataset.
        exclude_features:
          - processed_timestamp
          - data_split
          #- feature_date
          - user_pseudo_id
          - user_id
          - will_purchase
        pipeline_parameters_substitutions: null 
      prediction:
        name: "propensity-prediction-pl"
        job_id_prefix: "propensity-prediction-pl-"
        experiment_name: "propensity-prediction"
        # `type` can be "custom" or "tabular-workflows". 
        #  For using Vertex AI Tabular Workflow use the later, for all other modeling approaches use "custom" (i.e. BQML, Scikit-learn).
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 5 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${cloud_region}"
          job_name_prefix: "propensity-prediction-pl-"
          model_display_name: "propensity-training-pl-model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          model_metric_name: "logLoss"
          # The `model_metric_threshold` parameter defines what is the maximum acceptable value for the `model_metric_name` so that the model can be selected.
          # If the actual models metrics values are higher than this limit, no models will be selected and the pipeline is going to fail.
          model_metric_threshold: 0.9
          number_of_models_considered: 1
          # This is the prediction dataset table or view.
          bigquery_source: "${project_id}.purchase_propensity.v_purchase_propensity_inference_30_15"
          bigquery_destination_prefix: "${project_id}.purchase_propensity"
          bq_unique_key: "user_pseudo_id"
          machine_type: "n1-standard-4"
          max_replica_count: 10
          batch_size: 64
          accelerator_count: 0
          accelerator_type: "ACCELERATOR_TYPE_UNSPECIFIED" # ONE OF ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100, NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4
          generate_explanation: false
          # This is the probability value that will tell the condition to slit into the two classes.
          # For probabilities higher than `threashold`, set postive label to 1, otherwise 0.
          threashold: 0.5
          positive_label: "1"
          # These are parameters to invoke the aggregations of all daily predictions into a single table.
          aggregated_predictions_dataset_location: "${location}"
          query_aggregate_last_day_predictions: "CALL `${project_id}.aggregated_predictions.aggregate_last_day_predictions`();"
          # THese are parameters to trigger the Activation Application Dataflow.
          pubsub_activation_topic: "activation-trigger"
          pubsub_activation_type: "purchase-propensity-30-15"  # purchase-propensity-30-15 | purchase-propensity-15-15 | purchase-propensity-15-7" 
        pipeline_parameters_substitutions: null
    
    # This pipeline contains the configuration parameters for the segmentation training and inference pipelines for the audience segmentation model.
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the bigquery or Vertex AI KFP components to be used by your pipeline in `python/pipelines` section, if applicable.
    ## 3. Define or reuse the pipeline definition method to be used to compile the pipeline into a YAML file in `python/pipelines` section.
    ## 5. Create the terraform resources to compile, upload and schedule the pipeline in `terraform/pipelines/pipelines.tf`
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `python/pipelines/compiler.py` and `python/pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `python/pipelines/scheduler.py`.
    ## 9. Run terraform apply
    ## Note: For `type` = "tabular workflows", the pre-compiled YAML file `automl_tabular_pl_v4.yaml` is recompiled by parsing the `pipeline_parameters` below as default values
    ## to the the new YAML file. The recompiled YAML will be uploaded and scheduled in Vertex AI Pipelines.
    segmentation:
      training:
        name: "segmentation-training-pl"
        job_id_prefix: "segmentation-training-pl-"
        experiment_name: "segmentation-training"
        # `type` can be "custom" or "tabular-workflows". 
        #  For using Vertex AI Tabular Workflow use the later, for all other modeling approaches use "custom" (i.e. BQML, Scikit-learn).
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 12 * * SAT"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        # These are pipeline parameters that will be passed to the pipeline to be compiled
        # For Demographics Segmentation model, we use the BQML KMeans clustering algorithm.
        # Check the official documentation for better understanding the algorithm 
        # (https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-create-kmeans).
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          # The `km_num_clusters` parameter defines the number of clusters to be used during training.
          km_num_clusters: 10
          # The `km_init_method` parameter defines the BQML initialization method to be used
          km_init_method: "KMEANS++"
          km_distance_type: "EUCLIDEAN"
          km_standardize_features: "TRUE"
          km_max_interations: 20
          km_early_stop: "TRUE"
          km_min_rel_progress: 0.01
          km_warm_start: "FALSE"
          model_dataset_id: "${project_id}.audience_segmentation" # to also include project.dataset
          model_name_bq_prefix: "audience_segmentation_model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          vertex_model_name: "audience_segmentation_model"
          # This is the training dataset provided during the training routine.
          # The schema in this table or view must match the schema in the json files.
          # Take into consideration the `excluded_features` list below. They won't be used for training.
          training_data_bq_table: "${project_id}.audience_segmentation.v_audience_segmentation_training_15"
          # Features to be excluded from the training dataset.
          exclude_features:
            - processed_timestamp
            - data_split
            - user_pseudo_id
            - user_id
        pipeline_parameters_substitutions: null
      prediction:
        name: "segmentation-prediction-pl"
        job_id_prefix: "segmentation-prediction-pl-"
        experiment_name: "segmentation-prediction"
        # `type` can be "custom" or "tabular-workflows". 
        #  For using Vertex AI Tabular Workflow use the later, for all other modeling approaches use "custom" (i.e. BQML, Scikit-learn).
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 7 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          model_dataset_id: "${project_id}.audience_segmentation" # to also include project.dataset
          model_name_bq_prefix: "audience_segmentation_model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          model_metric_name: "davies_bouldin_index" # one of davies_bouldin_index ,  mean_squared_distance
          # This is the model metric value that will tell us if the model has good quality or not.
          # Lower index values indicate a better clustering result. The index is improved (lowered) by increased separation between clusters and decreased variation within clusters.
          # Check official documentation for better understanding 
          # (https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-syntax-evaluate#mlevaluate_output)
          model_metric_threshold: 10 
          number_of_models_considered: 2
          # This is the prediction dataset table or view.
          bigquery_source: "${project_id}.audience_segmentation.v_audience_segmentation_inference_15"
          bigquery_destination_prefix: "${project_id}.audience_segmentation.pred_audience_segmentation_inference_15"
          # These are parameters to invoke the aggregations of all daily predictions into a single table.
          aggregated_predictions_dataset_location: "${location}"
          query_aggregate_last_day_predictions: "CALL `${project_id}.aggregated_predictions.aggregate_last_day_predictions`();"
          # THese are parameters to trigger the Activation Application Dataflow.
          pubsub_activation_topic: "activation-trigger"
          pubsub_activation_type: "audience-segmentation-15"
        pipeline_parameters_substitutions: null

    # This pipeline contains the configuration parameters for the auto audience segmentation inference pipelines for the audience segmentation model.
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the bigquery or Vertex AI KFP components to be used by your pipeline in `python/pipelines` section, if applicable.
    ## 3. Define or reuse the pipeline definition method to be used to compile the pipeline into a YAML file in `python/pipelines` section.
    ## 5. Create the terraform resources to compile, upload and schedule the pipeline in `terraform/pipelines/pipelines.tf`
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `python/pipelines/compiler.py` and `python/pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `python/pipelines/scheduler.py`.
    ## 9. Run terraform apply
    ## Note: For `type` = "tabular workflows", the pre-compiled YAML file `automl_tabular_pl_v4.yaml` is recompiled by parsing the `pipeline_parameters` below as default values
    ## to the the new YAML file. The recompiled YAML will be uploaded and scheduled in Vertex AI Pipelines.
    auto_segmentation:
      training:
        name: "auto-segmentation-training-pl"
        job_id_prefix: "auto-segmentation-training-pl-"
        experiment_name: "auto-segmentation-training"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 12 * * SAT"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        pipeline_parameters:
          location: "${cloud_region}"
          project_id: "${project_id}"
          dataset: "auto_audience_segmentation"
          model_name: "interest-cluster-model"
          training_table: "auto_audience_segmentation_training_15"
          p_wiggle: 10
          min_num_clusters: 3
          bucket_name: "${project_id}-custom-models"
          image_uri: "us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-2:latest"
        pipeline_parameters_substitutions: null
      prediction:
        name: "auto-segmentation-prediction-pl"
        job_id_prefix: "auto-segmentation-prediction-pl-"
        experiment_name: "auto-segmentation-prediction"
        # `type` can be "custom" or "tabular-workflows". 
        #  For using Vertex AI Tabular Workflow use the later, for all other modeling approaches use "custom" (i.e. BQML, Scikit-learn).
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 7 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${cloud_region}"
          model_name: "interest-cluster-model"
          bigquery_source: "${project_id}.auto_audience_segmentation.v_auto_audience_segmentation_inference_15"
          bigquery_destination_prefix: "${project_id}.auto_audience_segmentation"
          # These are parameters to invoke the aggregations of all daily predictions into a single table.
          aggregated_predictions_dataset_location: "${location}"
          query_aggregate_last_day_predictions: "CALL `${project_id}.aggregated_predictions.aggregate_last_day_predictions`();"
          # These are parameters to trigger the Activation Application Dataflow.
          pubsub_activation_topic: "activation-trigger"
          pubsub_activation_type: "auto-audience-segmentation-15"
        pipeline_parameters_substitutions: null

    # This pipeline contains the configuration parameters for the purchase propensity model training pipelines used as part of the customer lifetime value (clv) inference pipeline.
    # The CLV training and inference pipeline requires a purchase propensity model training and a ltv regression model training. 
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the bigquery or Vertex AI KFP components to be used by your pipeline in `python/pipelines` section, if applicable.
    ## 3. Define or reuse the pipeline definition method to be used to compile the pipeline into a YAML file in `python/pipelines` section.
    ## 5. Create the terraform resources to compile, upload and schedule the pipeline in `terraform/pipelines/pipelines.tf`
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `python/pipelines/compiler.py` and `python/pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `python/pipelines/scheduler.py`.
    ## 9. Run terraform apply
    ## Note: For `type` = "tabular workflows", the pre-compiled YAML file `automl_tabular_pl_v4.yaml` is recompiled by parsing the `pipeline_parameters` below as default values
    ## to the the new YAML file. The recompiled YAML will be uploaded and scheduled in Vertex AI Pipelines.
    # This Propensity model will be used during the LTV prediction pipeline, the objective is to only predict revenue for the users who are more likely to purchase.
    # There is no need to have a specific prediction pipeline, since this model will be used in the prediction pipeline for the LTV.
    propensity_clv:
      training:
        name: "propensity-clv-training-pl"
        job_id_prefix: "propensity-clv-training-pl-"
        experiment_name: "propensity-clv-training"
        # `type` can be "custom" or "tabular-workflows". 
        #  For using Vertex AI Tabular Workflow use the later, for all other modeling approaches use "custom" (i.e. BQML, Scikit-learn).
        type: "tabular-workflows"
        schedule:
          cron: "TZ=America/New_York 0 16 * * SAT"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
          # These are pipeline parameters that will be passed to the pipeline to be recompiled
        pipeline_parameters:
          project: "${project_id}"
          location: "${cloud_region}"
          root_dir: "gs://${project_id}-pipelines/propensity-clv-training"
          transformations: "gs://${project_id}-pipelines/propensity-clv-training/transformations_config_{timestamp}.json"
          # These are specific data types transformations that will be applied to the dataset.
          custom_transformations: "pipelines/transformations-purchase-propensity-cltv.json"
          train_budget_milli_node_hours: 1000 # 1 hour
          # Set these to apply feature selection tuning.
          max_selected_features: 20
          apply_feature_selection_tuning: true
          run_evaluation: true
          run_distillation: false
          # Use `prediction_type` to "regression" for training models that predict a numerical value.  For classification models, use "classification" and you will
          # also get the probability likelihood for that class.
          prediction_type: "classification"
          # The optimization objectives may change depending on the `prediction_type`.
          # For binary classification, use "maximize-au-roc", "minimize-log-loss", "maximize-au-prc", "maximize-precision-at-recall" or "maximize-recall-at-precision". 
          # For multi class classification, use "minimize-log-loss". 
          # For regression, use "minimize-rmse", "minimize-mae", or "minimize-rmsle".
          optimization_objective: "maximize-au-roc"
          #optimization_objective_recall_value: 0.72
          #optimization_objective_precision_value: 0.72
          target_column: "will_purchase"
          predefined_split_key: "data_split"
          data_source_csv_filenames: null
          training_fraction: null
          validation_fraction: null
          test_fraction: null
          # This is the training dataset provided during the training routine.
          # The schema in this table or view must match the schema in the json files.
          # Take into consideration the `excluded_features` list below. They won't be used for training.
          data_source_bigquery_table_path: "bq://${project_id}.purchase_propensity.v_purchase_propensity_training_30_30_last_window"
          data_source_bigquery_table_schema: "../sql/schema/table/purchase_propensity_training_preparation.json"
          dataflow_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"
          timestamp_split_key: null
          stratified_split_key: null
          weight_column: null
          additional_experiments: null
          export_additional_model_without_custom_ops: false
          # Override the study spec parameters in case you want to restrict hyperparameter search space. Including `model_type`.
          #Don't use when parameter `apply_feature_selection_tuning` is `true`
          #study_spec_parameters_override:
          #  - parameter_id: "model_type"
          #    categorical_value_spec:
          #      values: 
          #        - nn
          #        - boosted_trees
          #  - parameter_id: "feature_selection_rate"
          #    double_value_spec:
          #      min_value: 0.5
          #      max_value: 1.0
          #    scale_type: UNIT_LINEAR_SCALE
        # Features to be excluded from the training dataset.
        exclude_features:
          - processed_timestamp
          - data_split
          #- feature_date
          - user_pseudo_id
          - user_id
          - will_purchase
        pipeline_parameters_substitutions: null
    
    # This pipeline contains the configuration parameters for the customer lifetime value training and inference pipelines for the customer lifetime value model.
    # To deploy this pipeline to your Google Cloud project:
    ## 1. Define the pipeline parameters below, following YAML format
    ## 2. Define the bigquery or Vertex AI KFP components to be used by your pipeline in `python/pipelines` section, if applicable.
    ## 3. Define or reuse the pipeline definition method to be used to compile the pipeline into a YAML file in `python/pipelines` section.
    ## 5. Create the terraform resources to compile, upload and schedule the pipeline in `terraform/pipelines/pipelines.tf`
    ## 6. Define python function that perform `compilation` and `upload to GCS bucket` are defined in `python/pipelines/compiler.py` and `python/pipelines/uploader.py`.
    ## 7. Define python function that perform `schedule` of the pipeline is defined in `python/pipelines/scheduler.py`.
    ## 9. Run terraform apply
    ## Note: For `type` = "tabular workflows", the pre-compiled YAML file `automl_tabular_pl_v4.yaml` is recompiled by parsing the `pipeline_parameters` below as default values
    ## to the the new YAML file. The recompiled YAML will be uploaded and scheduled in Vertex AI Pipelines.
    clv:
      # The Customer LTV value is by nature a regression model problem. However the challenge is that conversion rates in ecommerce scenarios are very low.
      # Typically, we see conversions rates around 1-5%, making it extremely hard for a regression model to predict it right.
      # The strategy adopted was to train a regression model with only customers features for those who bought, and use a propensity model
      # over the same look back and forward windows to predict whether the user is going to purchase or not.
      # If the user is not going to purchase then the LTV is zero(0), otherwise the LTV value is the one predicted by the regression model.
      training:
        name: "clv-training-pl"
        job_id_prefix: "clv-training-pl-"
        experiment_name: "clv-training"
        # `type` can be "custom" or "tabular-workflows". 
        #  For using Vertex AI Tabular Workflow use the later, for all other modeling approaches use "custom" (i.e. BQML, Scikit-learn).
        type: "tabular-workflows"
        schedule:
          cron: "TZ=America/New_York 0 20 * * SAT"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        # These are pipeline parameters that will be passed to the pipeline to be recompiled
        pipeline_parameters:
          project: "${project_id}"
          location: "${cloud_region}"
          root_dir: "gs://${project_id}-pipelines/clv-training"
          transformations: "gs://${project_id}-pipelines/clv-training/transformations_config_{timestamp}.json"
          custom_transformations: "pipelines/transformations-customer-ltv.json"
          train_budget_milli_node_hours: 1000 # 1 hour
          # Set these to apply feature selection tuning.
          max_selected_features: 20 
          apply_feature_selection_tuning: true
          run_evaluation: true
          run_distillation: false
          # Use `prediction_type` to "regression" for training models that predict a numerical value.  For classification models, use "classification" and you will
          # also get the probability likelihood for that class.
          prediction_type: "regression"
          target_column: "pltv_revenue_30_days"
          predefined_split_key: "data_split"
          training_fraction: null
          validation_fraction: null
          test_fraction: null
          data_source_csv_filenames: null
          # The optimization objectives may change depending on the `prediction_type`.
          # For binary classification, use "maximize-au-roc", "minimize-log-loss", "maximize-au-prc", "maximize-precision-at-recall" or "maximize-recall-at-precision". 
          # For multi class classification, use "minimize-log-loss". 
          # For regression, use "minimize-rmse", "minimize-mae", or "minimize-rmsle".
          optimization_objective: minimize-mae # minimize-mae | minimize-rmse | minimize-rmsle
          # This is the training dataset provided during the training routine.
          # The schema in this table or view must match the schema in the json files.
          # Take into consideration the `excluded_features` list below. They won't be used for training.
          data_source_bigquery_table_path: "bq://${project_id}.customer_lifetime_value.v_customer_lifetime_value_training_180_30_last_window"
          data_source_bigquery_table_schema: "../sql/schema/table/customer_lifetime_value_training_preparation.json"
          dataflow_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"
          timestamp_split_key: null
          stratified_split_key: null
          weight_column: null
          additional_experiments: null
          export_additional_model_without_custom_ops: false
          # Override the study spec parameters in case you want to restrict hyperparameter search space. Including `model_type`.
          #Don't use when parameter `apply_feature_selection_tuning` is `true`
          #study_spec_parameters_override:
          #  - parameter_id: "model_type"
          #    categorical_value_spec:
          #      values: 
          #        - nn
          #        - boosted_trees
          #  - parameter_id: "feature_selection_rate"
          #    double_value_spec:
          #      min_value: 0.5
          #      max_value: 1.0
          #    scale_type: UNIT_LINEAR_SCALE
        # Features to be excluded from the training dataset.
        exclude_features:
          - processed_timestamp
          - data_split
          #- feature_date
          - user_pseudo_id
          - user_id
        pipeline_parameters_substitutions: null 
      prediction:
        name: "clv-prediction-pl"
        job_id_prefix: "clv-prediction-pl-"
        experiment_name: "clv-prediction"
        # `type` can be "custom" or "tabular-workflows". 
        #  For using Vertex AI Tabular Workflow use the later, for all other modeling approaches use "custom" (i.e. BQML, Scikit-learn).
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 6 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: PAUSED # possible states ACTIVE or PAUSED
        # These are the pipeline parameters to be used in this convoluted prediction pipeline that takes predictions from LTV model and purchase propensity model.
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${cloud_region}"
          purchase_job_name_prefix: "propensity-prediction-pl-"
          clv_job_name_prefix: "clv-prediction-pl-"
          purchase_model_display_name: "propensity-clv-training-pl-model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          # The `purchase_model_metric_threshold` parameter defines what is the maximum acceptable value for the `purchase_model_metric_name` so that the model can be selected.
          # If the actual models metrics values are higher than this limit, no models will be selected and the pipeline is going to fail.
          purchase_model_metric_name: "logLoss"
          purchase_model_metric_threshold: 0.9
          number_of_purchase_models_considered: 1
          # This is the probability value that will tell the condition to slit into the two classes, purchasers and non-purchasers.
          # For probabilities higher than `threashold`, sets the LTV prediction pipeline to use the regression model predictions as the customer LTV value gain.
          # Probabilities lower than `threashold` sets the LTV prediction pipeline to use 0.0 as the customer LTV value gain.  
          threashold: 0.01
          positive_label: "1"
          clv_model_display_name: "clv-training-pl-model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          # The `purchase_model_metric_threshold` parameter defines what is the maximum acceptable value for the `purchase_model_metric_name` so that the model can be selected.
          # If the actual models metrics values are higher than this limit, no models will be selected and the pipeline is going to fail.
          clv_model_metric_name: "meanAbsoluteError" #'rootMeanSquaredError', 'meanAbsoluteError', 'meanAbsolutePercentageError', 'rSquared', 'rootMeanSquaredLogError'
          clv_model_metric_threshold: 400
          number_of_clv_models_considered: 1
          # This is the prediction dataset table or view for the purchase model.
          purchase_bigquery_source: "${project_id}.purchase_propensity.v_purchase_propensity_inference_30_30"
          purchase_bigquery_destination_prefix: "${project_id}.customer_lifetime_value"
          # This is the prediction dataset table or view for the CLV model.
          clv_bigquery_source: "${project_id}.customer_lifetime_value.v_customer_lifetime_value_inference_180_30"
          clv_bigquery_destination_prefix: "${project_id}.customer_lifetime_value"
          purchase_bq_unique_key: "user_pseudo_id"
          clv_bq_unique_key: "user_pseudo_id"
          machine_type: "n1-standard-4"
          max_replica_count: 10
          batch_size: 64
          accelerator_count: 0
          accelerator_type: "ACCELERATOR_TYPE_UNSPECIFIED" # ONE OF ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100, NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4
          generate_explanation: false
          # These are parameters to invoke the aggregations of all daily predictions into a single table.
          aggregated_predictions_dataset_location: "${location}"
          query_aggregate_last_day_predictions: "CALL `${project_id}.aggregated_predictions.aggregate_last_day_predictions`();"
          # THese are parameters to trigger the Activation Application Dataflow.
          pubsub_activation_topic: "activation-trigger"
          pubsub_activation_type: "cltv-180-30" # cltv-180-180 | cltv-180-90 | cltv-180-30
        pipeline_parameters_substitutions: null

# This block contains configuration parameters for the BigQuery Datasets, Tables, Queries and Stored Procedures.
bigquery:
  project_id: "${project_id}"
  region: "${location}"
  dataset:
    # Dataset for the feature engineering tables and procedures.
    feature_store:
      project_id: "${project_id}"
      name: "feature_store"
      location: "${location}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Feature Store dataset for Marketing behavioural modeling"
      friendly_name: "Feature Store"
      max_time_travel_hours: 168 
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    # Dataset for the purchase propensity use case.
    purchase_propensity:
      name: "purchase_propensity"
      location: "${location}"
      project_id: "${project_id}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Purchase Propensity Use Case dataset for Marketing behavioural modeling"
      friendly_name: "Purchase Propensity Dataset"
      max_time_travel_hours: 168
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    # Dataset for the customer lifetime value use case.
    customer_lifetime_value:
      project_id: "${project_id}"
      name: "customer_lifetime_value"
      location: "${location}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Customer Lifetime Value Use Case dataset for Marketing behavioural modeling"
      friendly_name: "Customer Lifetime Value Dataset"
      max_time_travel_hours: 168
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    # Dataset for the demographic based audience segmentation use case.
    audience_segmentation:
      project_id: "${project_id}"
      name: "audience_segmentation"
      location: "${location}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Audience Segmentation Use Case dataset for Marketing behavioural modeling"
      friendly_name: "Audience Segmentation Dataset"
      max_time_travel_hours: 168
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    # Dataset for the auto audience segmentation (Interests Based Audience Segmentation) use case.
    auto_audience_segmentation:
      project_id: "${project_id}"
      name: "auto_audience_segmentation"
      location: "${location}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Auto Audience Segmentation Use Case dataset for Marketing behavioural modeling"
      friendly_name: "Auto Audience Segmentation Dataset"
      max_time_travel_hours: 48
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    # Dataset for the aggregated Value Based Bidding (VBB) use case.
    aggregated_vbb:
      project_id: "${project_id}"
      name: "aggregated_vbb"
      location: "${location}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Aggregated VBB Use Case dataset for Marketing behavioural modeling"
      friendly_name: "Aggregated VBB Dataset"
      max_time_travel_hours: 48
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    # Dataset for the aggregated predictions tables and procedures.
    aggregated_predictions:
      project_id: "${project_id}"
      name: "aggregated_predictions"
      location: "${location}"
      description: "Dataset with aggregated prediction results from multiple use cases"
      friendly_name: "Aggregated Predictions Dataset"
  table:
    # Table containing the feature engineered dataset that will be used for the Audience Segmentation prediction pipeline.
    audience_segmentation_inference_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      table_name: "audience_segmentation_inference_preparation"
      location: "${location}"
      table_description: "Audience Segmentation Inference Preparation table to be used for Model Prediction"
    # Table containing the feature engineered dataset that will be used for the Customer Lifetime Value prediction pipeline.
    customer_lifetime_value_inference_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      table_name: "customer_lifetime_value_inference_preparation"
      location: "${location}"
      table_description: "Customer Lifetime Value Inference Preparation table to be used for Model Prediction"
    # Table containing the feature engineered labels that will be used for the Customer Lifetime Value training pipeline.
    customer_lifetime_value_label:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      table_name: "customer_lifetime_value_label"
      location: "${location}"
      table_description: "Customer Lifetime Value Label table to be used for Model Traning"
    # Table containing the feature engineered dataset that will be used for the Purchase Propensity prediction pipeline.
    purchase_propensity_inference_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      table_name: "purchase_propensity_inference_preparation"
      location: "${location}"
      table_description: "Purchase Propensity Inference Preparation table to be used for Model Prediction"
    # Table containing the feature engineered labels that will be used for the Purchase Propensity training pipeline.
    purchase_propensity_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "purchase_propensity_label"
      location: "${location}"
      table_description: "Purchase Propensity Label table to be used for Model Prediction"
    # Table containing the feature engineered dimensions that will be used for the Purchase Propensity training and inference pipeline.
    user_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_dimensions"
      location: "${location}"
      table_description: "User Dimensions table as part of the Feature Store for the Purchase Propensity use case"
    # Table containing the feature engineered dimensions that will be used for the Customer Lifetime Value training and inference pipeline.
    user_lifetime_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_lifetime_dimensions"
      location: "${location}"
      table_description: "User Lifetime Dimensions table as part of the Feature Store for the Customer Lifetime Value use case"
    # Table containing the feature engineered lookback rolling window metrics that will be used for the Audience Segmentation training and inference pipeline.
    user_lookback_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_lookback_metrics"
      location: "${location}"
      table_description: "User Lookback Metrics table as part of the Feature Store"
    # Table containing the feature engineered rolling window metrics that will be used for the Customer Lifetime Value training and inference pipeline.
    user_rolling_window_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_rolling_window_lifetime_metrics"
      location: "${location}"
      table_description: "User Rolling Window Lifetime Metrics table as part of the Feature Store for the Customer Lifetime Value use case" 
    # Table containing the featured engineered rolling window metrics that will be used for the Purchase Propensity training and inference pipeline.
    user_rolling_window_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_rolling_window_metrics"
      location: "${location}"
      table_description: "User Rolling Window Metrics table as part of the Feature Store for Purchase Propensity use case"
    # Table containing the feature engineered all users metrics that will be used for the Customer Lifetime Value training and inference pipeline.
    user_scoped_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_scoped_lifetime_metrics"
      location: "${location}"
      table_description: "User Scoped Lifetime Metrics table as part of the Feature Store for the Customer Lifetime Value use case"
    # Table containing the feature engineered all users metrics that will be used for the Purchase Propensity training and inference pipeline.
    user_scoped_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_scoped_metrics"
      location: "${location}"
      table_description: "User Scoped Metrics table as part of the Feature Store for the Purchase Propensity use case"
    # Table containing the feature engineered all users metrics that will be used for the Audience Segmentation training and inference pipeline.
    user_scoped_segmentation_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_scoped_segmentation_metrics"
      location: "${location}"
      table_description: "User Scoped Segmentation Metrics table as part of the Feature Store for Audience Segmentation use case" 
    # Table containing the feature engineered user dimensions that will be used for the Audience Segmentation training and inference pipeline.
    user_segmentation_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_segmentation_dimensions"
      location: "${location}"
      table_description: "User Segmentation Dimensions table as part of the Feature Store for Audience Segmentation use case"
    # Table containing the feature engineered user aggregated sessions and events metrics that will be used for the Purchase Propensity training and inference pipeline
    user_session_event_aggregated_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_session_event_aggregated_metrics"
      location: "${location}"
      table_description: "User Session Event Aggregated Metrics table as part of the Feature Store" 
  query:
    # This is a query template to be used by the Activation application, so there is no configuration to be applied.
    audience_segmentation_query_template:
      none: none
    # This is a query template to be used by the Activation application, so there is no configuration to be applied.
    auto_audience_segmentation_query_template:
      none: none
    # This is a query template to be used by the Activation application, so there is no configuration to be applied.
    purchase_propensity_query_template:
      none: none
    # This is a query template to be used by the Activation application, so there is no configuration to be applied.
    cltv_query_template:
      none: none
    # This is a stored procedure that CALLs the Aggregated Value Based Bidding Training Preparation stored procedure.
    invoke_aggregated_value_based_bidding_training_preparation:
      project_id: "${project_id}"
      dataset: "aggregated_vbb"
      stored_procedure: "aggregated_value_based_bidding_training_preparation"
    # This is a stored procedure that CALLs the Purchase Propensity Training Preparation stored procedure.
    invoke_purchase_propensity_training_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      stored_procedure: "purchase_propensity_training_preparation"
      # The `interval_max_date` parameter defines hwo many days we leave out of the training dataset after the latest date in the dataset.
      # This is usually the same value as the look forward window.
      interval_max_date: 15
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 30
      # `training_split_end_number` must be smaller then `validation_split_end_number`.
      # This is a number out of 10 deciles, how many rows will belong to the `data_split` = TRAIN (Between 1 and `training_split_end_number`)
      train_split_end_number: 5
      # This is a number out of 10 deciles, how many rows will belong to the `data_split` = VALIDATE (Between `training_split_end_number` and `validation_split_end_number`)
      # The rest of the rows will belong to the `data_split` = TEST (Between `validation_split_end_number` and 10)
      validation_split_end_number: 8
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is a stored procedure that CALLs the Audience Segmentation Training Preparation stored procedure.
    invoke_audience_segmentation_training_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      stored_procedure: "audience_segmentation_training_preparation"
      # The `interval_max_date` parameter defines hwo many days we leave out of the training dataset after the latest date in the dataset.
      # This is usually the same value as the look forward window.
      interval_max_date: 1
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 15
      # `training_split_end_number` must be smaller then `validation_split_end_number`.
      # This is a number out of 10 deciles, how many rows will belong to the `data_split` = TRAIN (Between 1 and `training_split_end_number`)
      train_split_end_number: 5
      # This is a number out of 10 deciles, how many rows will belong to the `data_split` = VALIDATE (Between `training_split_end_number` and `validation_split_end_number`)
      # The rest of the rows will belong to the `data_split` = TEST (Between `validation_split_end_number` and 10)
      validation_split_end_number: 8
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is a stored procedure that CALLs the Customer Lifetime Value Training Preparation stored procedure.
    invoke_customer_lifetime_value_training_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      stored_procedure: "customer_lifetime_value_training_preparation"
      # The `interval_max_date` parameter defines hwo many days we leave out of the training dataset after the latest date in the dataset.
      # This is usually the same value as the look forward window.
      interval_max_date: 180
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 180
      # `training_split_end_number` must be smaller then `validation_split_end_number`.
      # This is a number out of 10 deciles, how many rows will belong to the `data_split` = TRAIN (Between 1 and `training_split_end_number`)
      train_split_end_number: 5
      # This is a number out of 10 deciles, how many rows will belong to the `data_split` = VALIDATE (Between `training_split_end_number` and `validation_split_end_number`)
      # The rest of the rows will belong to the `data_split` = TEST (Between `validation_split_end_number` and 10)
      validation_split_end_number: 8
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}" 
    # This is a stored procedure that CALLs the Auto Audience Segmentation Training Preparation stored procedure.
    invoke_auto_audience_segmentation_training_preparation:
      project_id: "${project_id}"
      dataset: "auto_audience_segmentation"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      stored_procedure: "auto_audience_segmentation_training_preparation"
      # The `lookback_days` parameter is the number of days to look back for training data.
      lookback_days: 15
    # This is a stored procedure that CALLs the Purchase Propensity Training Inference Preparation stored procedure.
    invoke_purchase_propensity_inference_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      stored_procedure: "purchase_propensity_inference_preparation"
    # This is a stored procedure that CALLs the Customer Lifetime Value Inference Preparation stored procedure.
    invoke_customer_lifetime_value_inference_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      stored_procedure: "customer_lifetime_value_inference_preparation"
    # This is a stored procedure that CALLs the Audience Segmentation Inference Preparation stored procedure.
    invoke_audience_segmentation_inference_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      stored_procedure: "audience_segmentation_inference_preparation"
    # This is a stored procedure that CALLs the Auto Audience Segmentation Inference Preparation stored procedure.
    invoke_auto_audience_segmentation_inference_preparation:
      project_id: "${project_id}"
      dataset: "auto_audience_segmentation"
      stored_procedure: "auto_audience_segmentation_inference_preparation"
    # This is a stored procedure that CALLs the Aggregated Value Based Bidding Explanation Preparation stored procedure.
    invoke_aggregated_value_based_bidding_explanation_preparation:
      project_id: "${project_id}"
      dataset: "aggregated_vbb"
      stored_procedure: "aggregated_value_based_bidding_explanation_preparation"
    # This is a stored procedure that CALLs the User Lifetime Dimensions Backfill stored procedure.
    invoke_backfill_user_lifetime_dimensions:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_lifetime_dimensions"
      # The `interval_max_date` parameter defines hwo many days we leave out of the training dataset after the latest date in the dataset.
      # This is usually the same value as the look forward window.
      interval_max_date: 180
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 180
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180
    # This is a stored procedure that CALLs the User Scoped Lifetime Metrics Backfill stored procedure.
    invoke_backfill_user_scoped_lifetime_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}" 
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_scoped_lifetime_metrics"
      # The `interval_max_date` parameter defines hwo many days we leave out of the training dataset after the latest date in the dataset.
      # This is usually the same value as the look forward window.
      interval_max_date: 180
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 180
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180
    # This is a stored procedure that CALLs the User Session Event Aggregated Metrics Backfill stored procedure.
    invoke_backfill_user_session_event_aggregated_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}" 
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_session_event_aggregated_metrics"
      # The `interval_max_date` parameter defines hwo many days we leave out of the training dataset after the latest date in the dataset.
      # This is usually the same value as the look forward window.
      interval_max_date: 15
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 30
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 30
    # This is a stored procedure that CALLs the Customer Lifetime Value Label Backfill stored procedure.
    invoke_backfill_customer_lifetime_value_label:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "customer_lifetime_value_label"
      # The `interval_max_date` parameter defines hwo many days we leave out of the training dataset after the latest date in the dataset.
      # This is usually the same value as the look forward window.
      interval_max_date: 180
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 180
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180
    # This is a stored procedure that CALLs the User Lookback Metrics Backfill stored procedure.
    invoke_backfill_user_lookback_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_lookback_metrics"
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 15
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 15
    # This is a stored procedure that CALLs the User Rolling Window Lifetime Metrics Backfill stored procedure.
    invoke_backfill_user_rolling_window_lifetime_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_rolling_window_lifetime_metrics"
      # The `interval_max_date` parameter defines hwo many days we leave out of the training dataset after the latest date in the dataset.
      # This is usually the same value as the look forward window.
      interval_max_date: 180
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 180
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180
    # This is a stored procedure that CALLs the Purchase Propensity Label Backfill stored procedure.
    invoke_backfill_purchase_propensity_label:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "purchase_propensity_label"
      # The `interval_max_date` parameter defines hwo many days we leave out of the training dataset after the latest date in the dataset.
      # This is usually the same value as the look forward window.
      interval_max_date: 15
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 30
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 30
    # This is a stored procedure that CALLs the User Dimensions Backfill stored procedure.
    invoke_backfill_user_dimensions:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_dimensions"
      # The `interval_max_date` parameter defines hwo many days we leave out of the training dataset after the latest date in the dataset.
      # This is usually the same value as the look forward window.
      interval_max_date: 15
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 30
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 30
    # This is a stored procedure that CALLs the User Rolling Window Metrics Backfill stored procedure.
    invoke_backfill_user_rolling_window_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}" 
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_rolling_window_metrics"
      # The `interval_max_date` parameter defines hwo many days we leave out of the training dataset after the latest date in the dataset.
      # This is usually the same value as the look forward window.
      interval_max_date: 15
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 30
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 30
    # This is a stored procedure that CALLs the User Scoped Metrics Backfill stored procedure.
    invoke_backfill_user_scoped_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_scoped_metrics"
      # The `interval_max_date` parameter defines hwo many days we leave out of the training dataset after the latest date in the dataset.
      # This is usually the same value as the look forward window.
      interval_max_date: 15
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 30
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 30 
    # This is a stored procedure that CALLs the User Scoped Segmentation Metrics Backfill stored procedure.
    invoke_backfill_user_scoped_segmentation_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_scoped_segmentation_metrics"
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 15
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 15 
    # This is a stored procedure that CALLs the User Segmentation Dimensions Backfill stored procedure.
    invoke_backfill_user_segmentation_dimensions:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_segmentation_dimensions"
      # The `interval_min_date` parameter defines how many days we leave out of the training dataset before the first date in the dataset.
      # This is usually the same value as the lookback window.
      interval_min_date: 15
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 15
    # This is a stored procedure that CALLs the User Lifetime Value Label stored procedure.
    invoke_customer_lifetime_value_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "customer_lifetime_value_label"
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_input_date: 180
    # This is a stored procedure that CALLs the Purchase Propensity Label stored procedure.
    invoke_purchase_propensity_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "purchase_propensity_label"
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_input_date: 15
    # This is a stored procedure that CALLs the User Dimensions stored procedure.
    invoke_user_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_dimensions"
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180
    # This is a stored procedure that CALLs the User Lifetime Dimensions stored procedure.
    invoke_user_lifetime_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_lifetime_dimensions"
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180 
    # This is a stored procedure that CALLs the User Scoped Lifetime Metrics stored procedure.
    invoke_user_scoped_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_scoped_lifetime_metrics"
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180 
    # This is a stored procedure that CALLs the User Session Event Aggregated Metrics stored procedure.
    invoke_user_session_event_aggregated_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_session_event_aggregated_metrics"
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180
    # This is a stored procedure that CALLs the User Lookback Metrics stored procedure.
    invoke_user_lookback_metrics: 
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_lookback_metrics"
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180
    # This is a stored procedure that CALLs the User Rolling Window Lifetime Metrics stored procedure.
    invoke_user_rolling_window_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_rolling_window_lifetime_metrics"
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180
    # This is a stored procedure that CALLs the User Scoped Segmentation Metrics stored procedure.
    invoke_user_scoped_segmentation_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_scoped_segmentation_metrics"
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180
    # This is a stored procedure that CALLs the User Segmentation Dimensions stored procedure.
    invoke_user_segmentation_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_segmentation_dimensions"
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180
    # This is a stored procedure that CALLs the User Rolling Window Metrics stored procedure.
    invoke_user_rolling_window_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_rolling_window_metrics"
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180
    # This is a stored procedure that CALLs the User Scoped Metrics stored procedure.
    invoke_user_scoped_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_scoped_metrics"
      # The `interval_end_date` parameter defines how many days we leave out of the backfill before the last dates of events.
      # This is usually the same value as the look forward window.
      interval_end_date: 180
  # This section sets the parameters for the features, training and inference procedures that insert data into tables and views to be used for
  # training and prediction.
  # There is no strict recommendation on the right parameters that will maximize the models performance, however here are some back of the envelope numbers.
  #      Purchase Propensity model: 1 month-2 years for dates interval. From Xk - 10M users.
  #      Customer LTV model: 6 months-2 years for dates interval. From Xk - 10M users.
  #      Audience Segmentation / Auto Audience Segmentation models: 1 month-1 year for dates interval. From XXX - 10M users.
  #      Aggregated VBB model: 1000 days - 2000 days 
  #         Note: For Aggregated VBB, it's common to duplicate rows to that training dataset size reaches at least 1k rows for AutoML to train a model.
  #               If that is your case, this is not a problem since typically duplicated rows has a similar effect as of training the model for more epochs.
  procedure:
    # This is the stored procedure that collects the features and prepare the examples rows to train a model.
    # The procedure will split the data into three splits (TRAIN, VALIDATE, TEST) and will take care of avoiding splits contamination.
    # There is a minimum number of examples rows of 1000 and the maximum is as much as it fits in memory, overall consensus is that for ML models
    # you will provide at maximum a couple of millions of rows.
    # Note: For the Aggregated Value Based Bidding use case, you usually need less than 10000 rows returned qhen querying the view.
    aggregated_value_based_bidding_training_preparation:
      project_id: "${project_id}"
      dataset: "aggregated_vbb"
      name: "aggregated_value_based_bidding_training_preparation"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      view_name: "aggregated_value_based_bidding_training_full_dataset"
      expiration_duration_hours: 168
      # This is the datetime column name.
      datetime_column: "Dt"
      # Change the `eval_start_date` and `eval_end_date` in case you need your model to be validated in another interval of time.
      # The explanation metrics will be generated based on this subset.
      eval_start_date: "2024-01-01"
      eval_end_date: "2024-03-31"
    # This is the stored procedure that collects the features and prepare the examples rows to explain the features importances.
    # This procedure also prepares a few tables for reporting.
    aggregated_value_based_bidding_explanation_preparation:
      project_id: "${project_id}"
      dataset: "aggregated_vbb"
      name: "aggregated_value_based_bidding_explanation_preparation"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      # These are tables to be created to be used in the Looker Studio Report.
      volume_table_name: "aggregated_value_based_bidding_volume"
      corr_table_name: "aggregated_value_based_bidding_correlation"
      daily_volume_view_name: "aggregated_value_based_bidding_volume_daily"
      weekly_volume_view_name: "aggregated_value_based_bidding_volume_weekly"
      # This is the datetime column name.
      datetime_column: "Dt"
      # These are the `start_date` and `end_date` for which we're using the generate the tables above for Reporting.
      start_date: "'2024-01-01'"
      end_date: "NULL"
    # This is the stored procedure that collects the features and prepare the examples rows to train a model.
    # The procedure will split the data into three splits (TRAIN, VALIDATE, TEST) and will take care of avoiding splits contamination.
    # There is a minimum number of examples rows of 1000 and the maximum is as much as it fits in memory, overall consensus is that for ML models
    # you will provide at maximum a couple of millions of rows.
    audience_segmentation_training_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      name: "audience_segmentation_training_preparation"
      insert_table: "audience_segmentation_training_full_dataset"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      expiration_duration_hours: 168 
      # This parameter helps you set a maximum number of samples per split.
      samples_per_split: 100000
      custom_start_date: "'2023-01-01'"
      custom_end_date: "NULL"
    # This is the stored procedure that calculates the label column for the Customer Lifetime Value use case.
    # The label represents the lifetime value revenue gain over a period of time.
    # Typically, looking at a period of 30 days in the future.
    # The granularity level is per user per day.
    customer_lifetime_value_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "customer_lifetime_value_label"
      insert_table: "customer_lifetime_value_label"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is the stored procedure that collects the features and prepare the examples rows to train a model.
    # The procedure will split the data into three splits (TRAIN, VALIDATE, TEST) and will take care of avoiding splits contamination.
    # There is a minimum number of examples rows of 1000 and the maximum is as much as it fits in memory, overall consensus is that for ML models
    # you will provide at maximum a couple of millions of rows.
    customer_lifetime_value_training_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      name: "customer_lifetime_value_training_preparation"
      insert_table: "customer_lifetime_value_training_full_dataset"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      expiration_duration_hours: 168
      custom_start_date: "'2023-01-01'"
      custom_end_date: "NULL"
    # This is the stored procedure that calculates the label column for the Purchase Propensity use case.
    # The label represents wether a user will make a purchase over a period of time.
    # Typically, looking at a period of 15 to 30 days in the future.
    # The granularity level is per user per day.
    purchase_propensity_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "purchase_propensity_label"
      insert_table: "purchase_propensity_label"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is the stored procedure that collects the features and prepare the examples rows to train a model.
    # The procedure will split the data into three splits (TRAIN, VALIDATE, TEST) and will take care of avoiding splits contamination.
    # There is a minimum number of examples rows of 1000 and the maximum is as much as it fits in memory, overall consensus is that for ML models
    # you will provide at maximum a couple of millions of rows.
    purchase_propensity_training_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      name: "purchase_propensity_training_preparation"
      insert_table: "purchase_propensity_training_full_dataset"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      expiration_duration_hours: 168
      custom_start_date: "'2023-01-01'"
      custom_end_date: "NULL"
    # This is the stored procedure that UPSERTs new user dimensions rows daily. 
    # The granularity level is per user per day.
    # These dimensions are used for the Purchase Propensity use case.
    user_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_dimensions"
      insert_table: "user_dimensions"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is the stored procedure that UPSERTs new user dimensions rows daily. 
    # The granularity level is per user per day.
    # These dimensions are used for the Customer Lifetime Value use case.
    user_lifetime_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_lifetime_dimensions"
      insert_table: "user_lifetime_dimensions"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is the stored procedure that UPSERTs new look back rolling windows metrics rows daily.
    # The granularity level is per user per day.
    # These metrics are used for the Audience Segmentation use case.
    user_lookback_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_lookback_metrics"
      insert_table: "user_lookback_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is the stored procedure that UPSERTS new look back rolling windows metrics rows daily.
    # The granularity level is per user per day.
    # These metrics are used for the Customer Lifetime Value use case.
    user_rolling_window_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_rolling_window_lifetime_metrics"
      insert_table: "user_rolling_window_lifetime_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is the stored procedure that UPSERTs new look back rolling windows metrics rows daily.
    # The granularity level is per user per day.
    # These metrics are used for the Purchase Propensity use case.
    user_rolling_window_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_rolling_window_metrics"
      insert_table: "user_rolling_window_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is the stored procedure that UPSERTs new aggregated users metrics rows daily.
    # The granularity level is per day, whereas the calculations take into consideration all users.
    # These metrics are used for the Customer Lifetime Value use case.
    user_scoped_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_scoped_lifetime_metrics"
      insert_table: "user_scoped_lifetime_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is the stored procedure that UPSERTs new aggregated users metrics rows daily.
    # The granularity level is per day, whereas the calculations take into consideration all users.
    # These metrics are used for the Purchase Propensity use case.
    user_scoped_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_scoped_metrics"
      insert_table: "user_scoped_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is the stored procedure that UPSERTs new aggregated users metrics rows daily.
    # The granularity level is per day, whereas the calculations take into consideration all users.
    # These metrics are used for the Audience Segmentation use case.
    user_scoped_segmentation_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_scoped_segmentation_metrics"
      insert_table: "user_scoped_segmentation_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is the stored procedure that UPSERTs new user dimensions rows daily. 
    # The granularity level is per user per day.
    # These dimensions are used for the Audience Segmentation use case.
    user_segmentation_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_segmentation_dimensions"
      insert_table: "user_segmentation_dimensions"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is the stored procedure that UPSERTs new aggregated sessions and events metrics rows daily. 
    # The granularity level is per user per day.
    # These metrics are used for the Purchase Propensity use case.
    user_session_event_aggregated_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_session_event_aggregated_metrics"
      insert_table: "user_session_event_aggregated_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    # This is the stored procedure that collects the features and prepare the examples rows for daily prediction.
    purchase_propensity_inference_preparation:
      project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
      dataset: "purchase_propensity"
      name: "purchase_propensity_inference_preparation"
      feature_store_project_id: "${project_id}" 
      feature_store_dataset: "feature_store"
      insert_table: "purchase_propensity_inference_preparation"
      expiration_duration_hours: 168
    # This is the stored procedure that collects the features and prepare the examples rows for daily prediction.
    customer_lifetime_value_inference_preparation:
      project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
      dataset: "customer_lifetime_value"
      name: "customer_lifetime_value_inference_preparation"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      insert_table: "customer_lifetime_value_inference_preparation"
      expiration_duration_hours: 168
    # This is the stored procedure that collects the features and prepare the examples rows for daily prediction.
    audience_segmentation_inference_preparation:
      project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
      dataset: "audience_segmentation"
      name: "audience_segmentation_inference_preparation"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      insert_table: "audience_segmentation_inference_preparation"
      expiration_duration_hours: 168
    # This is the stored procedure that collects the features and prepare the examples rows for daily prediction.
    auto_audience_segmentation_inference_preparation:
      project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
      dataset: "auto_audience_segmentation"
      name: "auto_audience_segmentation_inference_preparation"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      insert_table: "auto_audience_segmentation_inference_preparation"
      lookback_days: 15
      expiration_duration_hours: 12
    # This is the stored procedure that collects the features and prepare the examples rows to train a model.
    # The procedure will split the data into three splits (TRAIN, VALIDATE, TEST) and will take care of avoiding splits contamination.
    # There is a minimum number of examples rows of 1000 and the maximum is as much as it fits in memory, overall consensus is that for ML models
    # you will provide at maximum a couple of millions of rows.
    auto_audience_segmentation_training_preparation:
      project_id: "${project_id}"
      mds_dataset: "${mds_dataset}"
      dataset: "auto_audience_segmentation"
      name: "auto_audience_segmentation_training_preparation"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      insert_table: "auto_audience_segmentation_training_preparation"
      lookback_days: 15
      expiration_duration_hours: 12
    # This procedure aggregates all the daily predictions generates into a consumable table grouped for each user.
    # This table is used for multiple Looker Studio dashboards reports. 
    aggregate_predictions_procedure:
      project_id: "${project_id}"
      dataset_id: "aggregated_predictions"
      table_id: "latest"
      customer_lifetime_value_dataset: "customer_lifetime_value"
      purchase_propensity_dataset: "purchase_propensity"
      audience_segmentation_dataset: "audience_segmentation"
      auto_audience_segmentation_dataset: "auto_audience_segmentation"

  
