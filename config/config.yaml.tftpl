# Copyright 2023 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# This *.tftpl file is a Terraform template file. It is used to create or update 
# resources in a Google Cloud project. A Terraform template file contains the 
# configuration parameters for the resources that you want to create or update. 
# It also contains the logic for creating or updating the resources. Terraform 
# template files are typically used to create or update resources that are 
# complex or that require a lot of configuration.
#
# The config.yaml.tftpl file is a YAML file that contains all the configuration 
# parameters for the marketing analytics jumpstart solution. 

# It contains the following sections:
#
# google_cloud_project: This section contains the Google Cloud project ID and project number.
# cloud_build: This section contains the configuration parameters for the Cloud Build pipeline.
# container: This section contains the configuration parameters for the container images.
# artifact_registry: This section contains the configuration parameters for the Artifact Registry repository.
# dataflow: This section contains the configuration parameters for the Dataflow pipeline.
# vertex_ai: This section contains the configuration parameters for the Vertex AI pipeline.
# bigquery: This section contains the configuration parameters for the BigQuery artifacts.

# This block contains general configuration parameters for the Google Cloud Project.
google_cloud_project:
  project_id: "${project_id}" # project_id terraform variable hidrated by terraform.
  project_name: "${project_name}" # project name terraform variable hidrated by terraform.
  project_number: "${project_number}" # project number terraform variable hidrated by terraform.
  region: "${cloud_region}" # region terraform variable hidrated by terraform.

# This block contains configuration parameters for the Cloud Build steps.
# The CI/CD pipelines are going to use Cloud Build to validate and test the solution at any time the
# developer wants.
# The Cloud Build pipeline will be created via a Terraform resource.
# This capability is still under construction, it is not ready to use.
cloud_build:
  project_id: "${project_id}"
  region: "${cloud_region}"
  github:
    owner: "${pipelines_github_owner}"
    repo_name: "${pipelines_github_repo}"
    trigger_branch: "dev"
  build_file: "cloudbuild/pipelines.yaml"
  _REPOSITORY_GCP_PROJECT: "${project_id}"
  _REPOSITORY_NAME: "github_${pipelines_github_owner}_${pipelines_github_repo}"
  _REPOSITORY_BRANCH: "main"
  _GCR_HOSTNAME: "${cloud_region}-docker.pkg.dev"
  _BUILD_REGION: "${cloud_region}"

# This block contains configuration parameters for the container images.
# The CI/CD pipelines are going to use the container parameters to auxiliate the Docker containers
# creation.
# The container images will be created via a Terraform resource.
# This capability is still under construction, it is not ready to use.
container:
  builder:
    # This is the base image used to run linting, formatting and unit tests of the python code.
    base:
      from_image: "python:3.7-alpine3.7"
      base_image_name: "base-builder"
      base_image_prefix: "maj"
    # This is the zetasql formatter image used to format and validate the SQL queries.
    zetasql:
      from_image: "wbsouza/zetasql-formatter:latest"
      base_image_name: "zetasql-formatter"
      base_image_prefix: "maj"
  container_registry_hostname: "${cloud_region}-docker.pkg.dev"
  container_registry_region: "${cloud_region}"

# This block contains configuration parameters for the Artifact Registry repositories.
# The Pipelines terraform module uses Artifact Registry to store the pipelines YAML configuration files
# and the docker images.
# Two repositories are created: one for the pipelines and one for the docker images.
# The pipelines repository is used to store the pipelines YAML configuration files to be compiled 
# and uploaded via a Terraform resource.
# The docker repository is used to store the Docker image built via a Terraform resource. 
# The only image being built right now is the pipeline components container image.
artifact_registry:
  pipelines_repo:
    name: "pipelines-repo"
    region: "${cloud_region}"
    project_id: "${project_id}"
  pipelines_docker_repo:
    name: "pipelines-docker-repo"
    region: "${cloud_region}"
    project_id: "${project_id}"

# This block contains configuration parameters for the Dataflow jobs and templates.
# The Activation Application terraform module uses Dataflow templates to create the Dataflow job.
# The Dataflow job is responsible for sending the model predictions to the GA4 / GAds platforms via
# Measurement Protocol API.
dataflow:
  # The `worker_service_account_id` is the service account ID used by the dataflow workers. 
  worker_service_account_id: "df-worker"
  # The `worker_service_account` is the service account used by the dataflow workers. 
  # The service account is created via a Terraform resource.
  worker_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"

# This block contains configuration parameters for the Vertex AI pipeline components.
# The Pipelines terraform module uses Vertex AI pipeline components to create the Vertex AI pipeline.
# The Vertex AI pipeline uses a Docker image at every step which is customized for this solution.
# Navigate to Vertex AI -> Training -> Custom Job to see the pipeline components executions.
vertex_ai:
  # The components block contains configuration parameters for the Vertex AI pipeline components.
  components:
    # The `base_image_name` is the name of the Docker container image used by the pipeline components.
    # The base image is created via a Terraform resource.
    # The Dockerfile recipe can be found at `pipelines/base_component_image/Dockerfile`.
    # Any python library dependencies should be installed in the `pyproject.toml` file.
    # The `pyproject.toml` file is a configuration file used by packaging tools, as well as other 
    # tools such as linters, type checkers, etc.
    # The `pyproject.toml` file can be found at `pipelines/base_component_image/pyproject.toml`.
    base_image_name: "ma-components"
    base_image_tag: "dev"

  # This pipelines block contains configuration parameters for the Vertex AI pipelines.
  # The current pipelines are:
  # - feature-creation-auto-audience-segmentation
  # - feature-creation-audience-segmentation
  # - feature-creation-purchase-propensity
  # - feature-creation-customer-ltv
  # - propensity
  #   - training
  #   - prediction
  # - segmentation
  #   - training
  #   - prediction
  # - auto_segmentation
  #   - training
  #   - prediction
  # - propensity_clv
  #   - training
  # - clv
  #   - training
  #   - prediction
  pipelines: 
    project_id: "${project_id}"
    service_account_id: "vertex-pipelines-sa"
    service_account: "vertex-pipelines-sa@${project_id}.iam.gserviceaccount.com"
    region: "${cloud_region}"
    bucket_name: "${project_id}-pipelines"
    model_bucket_name: "${project_id}-custom-models"
    root_path: "gs://${project_id}-pipelines/pipelines/"

    # This pipeline contains the configuration parameters for the feature creation pipeline for the auto audience segmentation model.
    # This block defines the pipeline parameters that are going to be used for three tasks: compilation, upload and scheduling.
    # The python function that perform `compilation` and `upload to GCS bucket` are defined in `python/pipelines/compiler.py` and `python/pipelines/uploader.py`.
    # The python function that `schedules`` the pipeline is defined in `python/pipelines/scheduler.py`.
    feature-creation-auto-audience-segmentation:
      execution:
        # The `name` parameter is the name of the pipeline that will appear in the Vertex AI pipeline UI.
        name: "feature-creation-auto-audience-segmentation"
        # The `job_id_prefix` is the prefix of the Vertex AI Custom Job that will be used at the execution of each individual component step.
        job_id_prefix: "feature-creation-auto-audience-segmentation-"
        # The `experiment_name` is the name of the experiment that will appear in the Vertex AI Experiments UI.
        experiment_name: "feature-creation-auto-audience-segmentation"
        # The `type` defines whether the pipeline is going to be a `tabular-workflows` or a `custom` pipeline.
        type: "custom"
        # The `schedule` defines the schedule values of the pipeline.
        # This solution uses the Vertex AI Pipeline Scheduler.
        # More information can be found at https://cloud.google.com/vertex-ai/docs/pipelines/scheduler.
        schedule:
          # The `cron` is the cron schedule. Make sure you review the TZ=America/New_York timezone.
          # More information can be found at https://cloud.google.com/scheduler/docs/configuring/cron-job-schedules.
          cron: "TZ=America/New_York 0 1 * * *"
          # The `max_concurrent_run_count` defines the maximum number of concurrent pipeline runs.
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          # The `state` defines the state of the pipeline.
          # In case you don't want to schedule the pipeline, set the state to `PAUSED`.
          state: ACTIVE # possible states ACTIVE or PAUSED
        # The `pipeline_parameters` defines the parameters that are going to be used to compile the pipeline.
        # Those values may difer depending on the pipeline type and the pipeline steps being used.
        # Make sure you review the python function the defines the pipeline.
        # The pipeline definition function can be found in `python/pipelines/feature_engineering_pipelines.py` 
        # or other files ending with `python/pipelines/*_pipeline.py`.
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          dataset: "auto_audience_segmentation"
          feature_table: "page_path_cumulative_traffic"
          mds_project_id: "${mds_project_id}"
          mds_dataset: "${mds_dataset}"
          date_start: "2023-01-01"
          date_end: "2023-12-31"
          stored_procedure_name: "auto_audience_segmentation_training_preparation"
          training_table: "auto_audience_segmentation_training_full_dataset"
          reg_expression: "^${website_url}([-a-zA-Z0-9@:%_+.~#?//=]*)$"
          perc_keep: 35
          lookback_days: 15
          query_auto_audience_segmentation_inference_preparation: "
            CALL `{auto_audience_segmentation_inference_preparation_procedure_name}`();"
          # The `timeout` parameter defines the timeout of the pipeline in seconds.
          # The default value is 3600 seconds (1 hour).
          timeout: 3600.0
        # The `pipeline_parameters_substitutions` defines the substitutions that are going to be applied to the pipeline parameters before compilation.
        # Check the parameter values above to see if they are used. 
        # They typically follow this format {parameter_subsititution_key}.
        # To apply a substitution, make sure you define the pair: {parameter_subsititution_key}: {parameter_subsititution_value}.
        pipeline_parameters_substitutions:
          date_timezone: "UTC" # used when input_date is None and need to get current date.
          auto_audience_segmentation_inference_preparation_procedure_name: "${project_id}.auto_audience_segmentation.invoke_auto_audience_segmentation_inference_preparation"
          auto_audience_segmentation_training_preparation_procedure_name: "${project_id}.auto_audience_segmentation.invoke_auto_audience_segmentation_training_preparation"
    
    # This pipeline contains the configuration parameters for the feature creation pipeline for the audience segmentation model.
    # This block defines the pipeline parameters that are going to be used for three tasks: compilation, upload and scheduling.
    # The python function that perform `compilation` and `upload to GCS bucket` are defined in `python/pipelines/compiler.py` and `python/pipelines/uploader.py`.
    # The python function that `schedules`` the pipeline is defined in `python/pipelines/scheduler.py`.
    feature-creation-audience-segmentation:
      execution:
        # The `name` parameter is the name of the pipeline that will appear in the Vertex AI pipeline UI.
        name: "feature-creation-audience-segmentation"
        # The `job_id_prefix` is the prefix of the Vertex AI Custom Job that will be used at the execution of each individual component step.
        job_id_prefix: "feature-creation-audience-segmentation-"
        # The `experiment_name` is the name of the experiment that will appear in the Vertex AI Experiments UI.
        experiment_name: "feature-creation-audience-segmentation"
        # The `type` defines whether the pipeline is going to be a `tabular-workflows` or a `custom` pipeline.
        type: "custom"
        # The `schedule` defines the schedule values of the pipeline.
        # This solution uses the Vertex AI Pipeline Scheduler.
        # More information can be found at https://cloud.google.com/vertex-ai/docs/pipelines/scheduler.
        schedule:
          # The `cron` is the cron schedule. Make sure you review the TZ=America/New_York timezone.
          # More information can be found at https://cloud.google.com/scheduler/docs/configuring/cron-job-schedules.
          cron: "TZ=America/New_York 0 1 * * *"
          # The `max_concurrent_run_count` defines the maximum number of concurrent pipeline runs.
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          # The `state` defines the state of the pipeline.
          # In case you don't want to schedule the pipeline, set the state to `PAUSED`.
          state: ACTIVE # possible states ACTIVE or PAUSED
        # The `pipeline_parameters` defines the parameters that are going to be used to compile the pipeline.
        # Those values may difer depending on the pipeline type and the pipeline steps being used.
        # Make sure you review the python function the defines the pipeline.
        # The pipeline definition function can be found in `python/pipelines/feature_engineering_pipelines.py` 
        # or other files ending with `python/pipelines/*_pipeline.py`.
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          query_user_segmentation_dimensions: "
            CALL `{user_segmentation_dimensions_procedure_name}`();"
          query_user_lookback_metrics: "
            CALL `{user_lookback_metrics_procedure_name}`();"
          query_user_scoped_segmentation_metrics: "
            CALL `{user_scoped_segmentation_metrics_procedure_name}`();"
          query_audience_segmentation_inference_preparation: "
            CALL `{audience_segmentation_inference_preparation_procedure_name}`();"
          query_audience_segmentation_training_preparation: "
            CALL `{audience_segmentation_training_preparation_procedure_name}`();"
          # The `timeout` parameter defines the timeout of the pipeline in seconds.
          # The default value is 3600 seconds (1 hour).
          timeout: 3600.0
        # The `pipeline_parameters_substitutions` defines the substitutions that are going to be applied to the pipeline parameters before compilation.
        # Check the parameter values above to see if they are used. 
        # They typically follow this format {parameter_subsititution_key}.
        # To apply a substitution, make sure you define the pair: {parameter_subsititution_key}: {parameter_subsititution_value}.
        pipeline_parameters_substitutions: # Substitutions are applied to the parameters before compilation
          user_segmentation_dimensions_procedure_name: "${project_id}.feature_store.invoke_user_segmentation_dimensions"
          user_lookback_metrics_procedure_name: "${project_id}.feature_store.invoke_user_lookback_metrics"
          user_scoped_segmentation_metrics_procedure_name: "${project_id}.feature_store.invoke_user_scoped_segmentation_metrics"
          date_timezone: "UTC" # used when input_date is None and need to get current date.
          audience_segmentation_inference_preparation_procedure_name: "${project_id}.audience_segmentation.invoke_audience_segmentation_inference_preparation"
          audience_segmentation_training_preparation_procedure_name: "${project_id}.audience_segmentation.invoke_audience_segmentation_training_preparation"

    # This pipeline contains the configuration parameters for the feature creation pipeline for the purchase propensity model.
    feature-creation-purchase-propensity: 
      execution:
        name: "feature-creation-purchase-propensity"
        job_id_prefix: "feature-creation-purchase-propensity-"
        experiment_name: "feature-creation-purchase-propensity"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 1 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          query_purchase_propensity_label: "
            CALL `{purchase_propensity_label_procedure_name}`();"
          query_user_dimensions: "
            CALL `{user_dimensions_procedure_name}`();"
          query_user_rolling_window_metrics: "
            CALL `{user_rolling_window_metrics_procedure_name}`();"
          query_user_scoped_metrics: "
            CALL `{user_scoped_metrics_procedure_name}`();"
          query_user_session_event_aggregated_metrics: "
            CALL `{user_session_event_aggregated_metrics_procedure_name}`();"
          query_purchase_propensity_inference_preparation: "
            CALL `{purchase_propensity_inference_preparation_procedure_name}`();"
          query_purchase_propensity_training_preparation: "
            CALL `{purchase_propensity_training_preparation_procedure_name}`();"
          timeout: 3600.0
        pipeline_parameters_substitutions: # Substitutions are applied to the parameters before compilation
          purchase_propensity_label_procedure_name: "${project_id}.feature_store.invoke_purchase_propensity_label"
          user_dimensions_procedure_name: "${project_id}.feature_store.invoke_user_dimensions"
          user_rolling_window_metrics_procedure_name: "${project_id}.feature_store.invoke_user_rolling_window_metrics"
          user_scoped_metrics_procedure_name: "${project_id}.feature_store.invoke_user_scoped_metrics"
          user_session_event_aggregated_metrics_procedure_name: "${project_id}.feature_store.invoke_user_session_event_aggregated_metrics"
          date_timezone: "UTC" # used when input_date is None and need to get current date.
          purchase_propensity_inference_preparation_procedure_name: "${project_id}.purchase_propensity.invoke_purchase_propensity_inference_preparation"
          purchase_propensity_training_preparation_procedure_name: "${project_id}.purchase_propensity.invoke_purchase_propensity_training_preparation"

    # This pipeline contains the configuration parameters for the feature creation pipeline for the customer lifetime value model.
    feature-creation-customer-ltv:
      execution:
        name: "feature-creation-customer-ltv"
        job_id_prefix: "feature-creation-customer-ltv-"
        experiment_name: "feature-creation-customer-ltv"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 1 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          query_customer_lifetime_value_label: "
            CALL `{customer_lifetime_value_label_procedure_name}`();"
          query_user_lifetime_dimensions: "
            CALL `{user_lifetime_dimensions_procedure_name}`();"
          query_user_rolling_window_lifetime_metrics: "
            CALL `{user_rolling_window_lifetime_metrics_procedure_name}`();"
          query_user_scoped_lifetime_metrics: "
            CALL `{user_scoped_lifetime_metrics_procedure_name}`();"
          query_customer_lifetime_value_inference_preparation: "
            CALL `{customer_lifetime_value_inference_preparation_procedure_name}`();"
          query_customer_lifetime_value_training_preparation: "
            CALL `{customer_lifetime_value_training_preparation_procedure_name}`();"
          timeout: 3600.0
        pipeline_parameters_substitutions: # Substitutions are applied to the parameters before compilation
          customer_lifetime_value_label_procedure_name: "${project_id}.feature_store.invoke_customer_lifetime_value_label"
          user_lifetime_dimensions_procedure_name: "${project_id}.feature_store.invoke_user_lifetime_dimensions"
          user_rolling_window_lifetime_metrics_procedure_name: "${project_id}.feature_store.invoke_user_rolling_window_lifetime_metrics"
          user_scoped_lifetime_metrics_procedure_name: "${project_id}.feature_store.invoke_user_scoped_lifetime_metrics"
          date_timezone: "UTC" # used when input_date is None and need to get current date.
          customer_lifetime_value_inference_preparation_procedure_name: "${project_id}.customer_lifetime_value.invoke_customer_lifetime_value_inference_preparation"
          customer_lifetime_value_training_preparation_procedure_name: "${project_id}.customer_lifetime_value.invoke_customer_lifetime_value_training_preparation"

    feature-creation-aggregated-value-based-bidding:
      execution:
        name: "feature-creation-aggregated-value-based-bidding"
        job_id_prefix: "feature-creation-aggregated-value-based-bidding-"
        experiment_name: "feature-creation-aggregated-value-based-bidding"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 1 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          query_aggregated_value_based_bidding_training_preparation: "
            CALL `{aggregated_value_based_bidding_training_preparation_procedure_name}`();"
          timeout: 3600
        pipeline_parameters_substitutions:
          aggregated_value_based_bidding_training_preparation_procedure_name: "${project_id}.aggregated_vbb.invoke_aggregated_value_based_bidding_training_preparation"
    
    # This pipeline contains the configuration parameters for the value based bidding training and inference pipelines.
    value_based_bidding:
      training:
        name: "value-based-bidding-training-pl"
        job_id_prefix: "value-based-bidding-training-pl-"
        experiment_name: "value-based-bidding-training"
        type: "tabular-workflows"
        schedule:
          # define the schedule for the pipeline
          cron: "TZ=America/New_York 0 1 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED 
        pipeline_parameters:
          project: "${project_id}"
          location: "${cloud_region}"
          root_dir: "gs://${project_id}-pipelines/value-based-bidding"
          transformations: "gs://${project_id}-pipelines/value-based-bidding/transformations_config_{timestamp}.json"
          train_budget_milli_node_hours: 1000 # 1 hour
          run_evaluation: true
          run_distillation: false
          prediction_type: "regression"
          optimization_objective: "minimize-mae" #minimize-rmse
          target_column: "Purchase_Product"
          predefined_split_key: "data_split"
          data_source_csv_filenames: null
          training_fraction: null
          validation_fraction: null
          test_fraction: null
          # data_source_bigquery_table_path: "bq://${project_id}.aggregated_vbb.aggregated_vbb_training""
          data_source_bigquery_table_path: "bq://${project_id}.aggregated_vbb.aggregated_value_based_bidding_training_full_dataset"
          data_source_bigquery_table_schema: "../sql/schema/table/value_based_bidding_training_preparation.json"
          dataflow_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"
          timestamp_split_key: null
          stratified_split_key: null
          weight_column: null
          additional_experiments: null
          export_additional_model_without_custom_ops: false
          study_spec_parameters_override:
            - parameter_id: "model_type"
              categorical_value_spec:
                values: 
                  - boosted_trees
        exclude_features:
          - Purchase_Product
          - Dt
          - data_split
        pipeline_parameters_substitutions: null
      explanation:
        name: "value-based-bidding-explanation-pl"
        job_id_prefix: "value-based-bidding-explanation-pl-"
        experiment_name: "value-based-bidding-explanation"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 5 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project: "${project_id}"
          location: "${cloud_region}"
          data_location: "${location}"
          model_display_name: "value-based-bidding-training-pl-model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          model_metric_name: "meanAbsoluteError" #'rootMeanSquaredError', 'meanAbsoluteError', 'meanAbsolutePercentageError', 'rSquared', 'rootMeanSquaredLogError'
          model_metric_threshold: 400
          number_of_models_considered: 1
          bigquery_destination_prefix: "${project_id}.aggregated_vbb.vbb_weights"
        pipeline_parameters_substitutions: null

    # This pipeline contains the configuration parameters for the propensity training and inference pipelines for the purchase propensity model.
    propensity:
      training:
        name: "propensity-training-pl"
        job_id_prefix: "propensity-training-pl-"
        experiment_name: "propensity-training"
        type: "tabular-workflows"
        schedule:
          cron: "TZ=America/New_York 0 8 * * SAT"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project: "${project_id}"
          location: "${cloud_region}"
          root_dir: "gs://${project_id}-pipelines/propensity-training"
          transformations: "gs://${project_id}-pipelines/propensity-training/transformations_config_{timestamp}.json"
          custom_transformations: "pipelines/transformations-purchase-propensity.json"
          train_budget_milli_node_hours: 1000 # 1 hour
          max_selected_features: 20
          apply_feature_selection_tuning: true
          run_evaluation: true
          run_distillation: false
          prediction_type: "classification"
          optimization_objective: "minimize-log-loss"
          target_column: "will_purchase"
          predefined_split_key: "data_split"
          data_source_csv_filenames: null
          training_fraction: null
          validation_fraction: null
          test_fraction: null
          # data_source_bigquery_table_path: "bq://${project_id}.purchase_propensity.v_purchase_propensity_training_15_7"
          # data_source_bigquery_table_path: "bq://${project_id}.purchase_propensity.v_purchase_propensity_training_15_15"
          data_source_bigquery_table_path: "bq://${project_id}.purchase_propensity.v_purchase_propensity_training_30_15_balanced"
          data_source_bigquery_table_schema: "../sql/schema/table/purchase_propensity_training_preparation.json"
          dataflow_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"
          timestamp_split_key: null
          stratified_split_key: null
          weight_column: null
          additional_experiments: null
          export_additional_model_without_custom_ops: false
          #Don't use when parameter `apply_feature_selection_tuning` is `true`
          #study_spec_parameters_override:
          #  - parameter_id: "model_type"
          #    categorical_value_spec:
          #      values: 
          #        - nn
          #        - boosted_trees
          #  - parameter_id: "feature_selection_rate"
          #    double_value_spec:
          #      min_value: 0.5
          #      max_value: 1.0
          #    scale_type: UNIT_LINEAR_SCALE
        exclude_features:
          - processed_timestamp
          - data_split
          - feature_date
          - user_pseudo_id
          - user_id
          - will_purchase
        pipeline_parameters_substitutions: null 
      prediction:
        name: "propensity-prediction-pl"
        job_id_prefix: "propensity-prediction-pl-"
        experiment_name: "propensity-prediction"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 5 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${cloud_region}"
          job_name_prefix: "propensity-prediction-pl-"
          model_display_name: "propensity-training-pl-model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          model_metric_name: "logLoss"
          model_metric_threshold: 0.9
          number_of_models_considered: 1
          bigquery_source: "${project_id}.purchase_propensity.v_purchase_propensity_inference_30_15"
          bigquery_destination_prefix: "${project_id}.purchase_propensity"
          bq_unique_key: "user_pseudo_id"
          machine_type: "n1-standard-4"
          max_replica_count: 10
          batch_size: 64
          accelerator_count: 0
          accelerator_type: "ACCELERATOR_TYPE_UNSPECIFIED" # ONE OF ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100, NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4
          generate_explanation: false
          threashold: 0.5
          positive_label: "1"
          aggregated_predictions_dataset_location: "${location}"
          query_aggregate_last_day_predictions: "CALL `${project_id}.aggregated_predictions.aggregate_last_day_predictions`();"
          pubsub_activation_topic: "activation-trigger"
          pubsub_activation_type: "purchase-propensity-30-15"  # purchase-propensity-30-15 | purchase-propensity-15-15 | purchase-propensity-15-7" 
        pipeline_parameters_substitutions: null
    
    # This pipeline contains the configuration parameters for the segmentation training and inference pipelines for the audience segmentation model.
    segmentation:
      training:
        name: "segmentation-training-pl"
        job_id_prefix: "segmentation-training-pl-"
        experiment_name: "segmentation-training"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 12 * * SAT"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          km_num_clusters: 14
          km_init_method: "KMEANS++"
          km_distance_type: "EUCLIDEAN"
          km_standardize_features: "TRUE"
          km_max_interations: 20
          km_early_stop: "TRUE"
          km_min_rel_progress: 0.01
          km_warm_start: "FALSE"
          model_dataset_id: "${project_id}.audience_segmentation" # to also include project.dataset
          model_name_bq_prefix: "audience_segmentation_model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          vertex_model_name: "audience_segmentation_model"
          training_data_bq_table: "${project_id}.audience_segmentation.v_audience_segmentation_training_15"
          exclude_features:
            - processed_timestamp
            - data_split
            - feature_date
            - user_pseudo_id
            - user_id
        pipeline_parameters_substitutions: null
      prediction:
        name: "segmentation-prediction-pl"
        job_id_prefix: "segmentation-prediction-pl-"
        experiment_name: "segmentation-prediction"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 7 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${location}"
          model_dataset_id: "${project_id}.audience_segmentation" # to also include project.dataset
          model_name_bq_prefix: "audience_segmentation_model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          model_metric_name: "davies_bouldin_index" # one of davies_bouldin_index ,  mean_squared_distance
          model_metric_threshold: 10 
          number_of_models_considered: 2
          bigquery_source: "${project_id}.audience_segmentation.v_audience_segmentation_inference_15"
          bigquery_destination_prefix: "${project_id}.audience_segmentation.pred_audience_segmentation_inference_15"
          aggregated_predictions_dataset_location: "${location}"
          query_aggregate_last_day_predictions: "CALL `${project_id}.aggregated_predictions.aggregate_last_day_predictions`();"
          pubsub_activation_topic: "activation-trigger"
          pubsub_activation_type: "audience-segmentation-15" # audience-segmentation-15
        pipeline_parameters_substitutions: null

    # This pipeline contains the configuration parameters for the auto audience segmentation inference pipelines for the audience segmentation model.
    auto_segmentation:
      training:
        name: "auto-segmentation-training-pl"
        job_id_prefix: "auto-segmentation-training-pl-"
        experiment_name: "auto-segmentation-training"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 12 * * SAT"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          location: "${cloud_region}"
          project_id: "${project_id}"
          dataset: "auto_audience_segmentation"
          model_name: "interest-cluster-model"
          training_table: "auto_audience_segmentation_training_full_dataset"
          p_wiggle: 10
          min_num_clusters: 3
          bucket_name: "${project_id}-custom-models"
          image_uri: "us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-2:latest"
        pipeline_parameters_substitutions: null
      prediction:
        name: "auto-segmentation-prediction-pl"
        job_id_prefix: "auto-segmentation-prediction-pl-"
        experiment_name: "auto-segmentation-prediction"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 7 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${cloud_region}"
          model_name: "interest-cluster-model"
          bigquery_source: "${project_id}.auto_audience_segmentation.v_auto_audience_segmentation_inference_15"
          #bigquery_destination_prefix: "${project_id}.auto_audience_segmentation.pred_auto_audience_segmentation_inference_15"
          bigquery_destination_prefix: "${project_id}.auto_audience_segmentation"
          pubsub_activation_topic: "activation-trigger"
          pubsub_activation_type: "auto-audience-segmentation-15"
        pipeline_parameters_substitutions: null

    # This pipeline contains the configuration parameters for the purchase propensity model training pipelines used as part of the customer lifetime value (clv) inference pipeline.
    # The CLV training and inference pipeline requires a purchase propensity model training and a ltv regression model training. 
    propensity_clv:
      training:
        name: "propensity-clv-training-pl"
        job_id_prefix: "propensity-clv-training-pl-"
        experiment_name: "propensity-clv-training"
        type: "tabular-workflows"
        schedule:
          cron: "TZ=America/New_York 0 16 * * SAT"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project: "${project_id}"
          location: "${cloud_region}"
          root_dir: "gs://${project_id}-pipelines/propensity-clv-training"
          transformations: "gs://${project_id}-pipelines/propensity-clv-training/transformations_config_{timestamp}.json"
          train_budget_milli_node_hours: 1000 # 1 hour
          max_selected_features: 20
          apply_feature_selection_tuning: true
          run_evaluation: true
          run_distillation: false
          prediction_type: "classification"
          optimization_objective: "minimize-log-loss"
          target_column: "will_purchase"
          predefined_split_key: "data_split"
          data_source_csv_filenames: null
          training_fraction: null
          validation_fraction: null
          test_fraction: null
          data_source_bigquery_table_path: "bq://${project_id}.purchase_propensity.v_purchase_propensity_training_30_30_balanced"
          data_source_bigquery_table_schema: "../sql/schema/table/purchase_propensity_training_preparation.json"
          dataflow_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"
          timestamp_split_key: null
          stratified_split_key: null
          weight_column: null
          additional_experiments: null
          export_additional_model_without_custom_ops: false
          #Don't use when parameter `apply_feature_selection_tuning` is `true`
          #study_spec_parameters_override:
          #  - parameter_id: "model_type"
          #    categorical_value_spec:
          #      values: 
          #        - nn
          #        - boosted_trees
          #  - parameter_id: "feature_selection_rate"
          #    double_value_spec:
          #      min_value: 0.5
          #      max_value: 1.0
          #    scale_type: UNIT_LINEAR_SCALE
        exclude_features:
          - processed_timestamp
          - data_split
          - feature_date
          - user_pseudo_id
          - user_id
          - will_purchase
        pipeline_parameters_substitutions: null
    
    # This pipeline contains the configuration parameters for the customer lifetime value training and inference pipelines for the customer lifetime value model.
    clv:
      training:
        name: "clv-training-pl"
        job_id_prefix: "clv-training-pl-"
        experiment_name: "clv-training"
        type: "tabular-workflows"
        schedule:
          cron: "TZ=America/New_York 0 20 * * SAT"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project: "${project_id}"
          location: "${cloud_region}"
          root_dir: "gs://${project_id}-pipelines/clv-training"
          transformations: "gs://${project_id}-pipelines/clv-training/transformations_config_{timestamp}.json"
          custom_transformations: "pipelines/transformations-customer-ltv.json"
          train_budget_milli_node_hours: 1000 # 1 hour
          max_selected_features: 20 
          apply_feature_selection_tuning: true
          run_evaluation: true
          run_distillation: false
          prediction_type: "regression"
          target_column: "pltv_revenue_30_days"
          predefined_split_key: "data_split"
          training_fraction: null
          validation_fraction: null
          test_fraction: null
          data_source_csv_filenames: null
          optimization_objective: minimize-mae # minimize-mae | minimize-rmse | minimize-rmsle
          data_source_bigquery_table_path: "bq://${project_id}.customer_lifetime_value.v_customer_lifetime_value_training_180_30"
          data_source_bigquery_table_schema: "../sql/schema/table/customer_lifetime_value_training_preparation.json"
          dataflow_service_account: "df-worker@${project_id}.iam.gserviceaccount.com"
          timestamp_split_key: null
          stratified_split_key: null
          weight_column: null
          additional_experiments: null
          export_additional_model_without_custom_ops: false
          #Don't use when parameter `apply_feature_selection_tuning` is `true`
          #study_spec_parameters_override:
          #  - parameter_id: "model_type"
          #    categorical_value_spec:
          #      values: 
          #        - nn
          #        - boosted_trees
          #  - parameter_id: "feature_selection_rate"
          #    double_value_spec:
          #      min_value: 0.5
          #      max_value: 1.0
          #    scale_type: UNIT_LINEAR_SCALE
        exclude_features:
          - processed_timestamp
          - data_split
          - feature_date
          - user_pseudo_id
          - user_id
        pipeline_parameters_substitutions: null 
      prediction:
        name: "clv-prediction-pl"
        job_id_prefix: "clv-prediction-pl-"
        experiment_name: "clv-prediction"
        type: "custom"
        schedule:
          cron: "TZ=America/New_York 0 6 * * *"
          max_concurrent_run_count: 1
          start_time: null
          end_time: null
          state: ACTIVE # possible states ACTIVE or PAUSED
        pipeline_parameters:
          project_id: "${project_id}"
          location: "${cloud_region}"
          purchase_job_name_prefix: "propensity-prediction-pl-"
          clv_job_name_prefix: "clv-prediction-pl-"
          purchase_model_display_name: "propensity-clv-training-pl-model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          purchase_model_metric_name: "logLoss"
          purchase_model_metric_threshold: 0.9
          number_of_purchase_models_considered: 1
          threashold: 0.5
          positive_label: "1"
          clv_model_display_name: "clv-training-pl-model" # must match the model name defined in the training pipeline. for now it is {NAME_OF_PIPELINE}-model
          clv_model_metric_name: "meanAbsoluteError" #'rootMeanSquaredError', 'meanAbsoluteError', 'meanAbsolutePercentageError', 'rSquared', 'rootMeanSquaredLogError'
          clv_model_metric_threshold: 400
          number_of_clv_models_considered: 1
          purchase_bigquery_source: "${project_id}.purchase_propensity.v_purchase_propensity_inference_30_30"
          purchase_bigquery_destination_prefix: "${project_id}.customer_lifetime_value"
          clv_bigquery_source: "${project_id}.customer_lifetime_value.v_customer_lifetime_value_inference_180_30"
          clv_bigquery_destination_prefix: "${project_id}.customer_lifetime_value"
          purchase_bq_unique_key: "user_pseudo_id"
          clv_bq_unique_key: "user_pseudo_id"
          machine_type: "n1-standard-4"
          max_replica_count: 10
          batch_size: 64
          accelerator_count: 0
          accelerator_type: "ACCELERATOR_TYPE_UNSPECIFIED" # ONE OF ACCELERATOR_TYPE_UNSPECIFIED, NVIDIA_TESLA_K80, NVIDIA_TESLA_P100, NVIDIA_TESLA_V100, NVIDIA_TESLA_P4, NVIDIA_TESLA_T4
          generate_explanation: false
          aggregated_predictions_dataset_location: "${location}"
          query_aggregate_last_day_predictions: "CALL `${project_id}.aggregated_predictions.aggregate_last_day_predictions`();"
          pubsub_activation_topic: "activation-trigger"
          pubsub_activation_type: "cltv-180-30" # cltv-180-180 | cltv-180-90 | cltv-180-30
        pipeline_parameters_substitutions: null

# This block contains configuration parameters for the BigQuery Datasets, Tables, Queries and Stored Procedures.
bigquery:
  project_id: "${project_id}"
  region: "${location}"
  dataset:
    feature_store:
      project_id: "${project_id}"
      name: "feature_store"
      location: "${location}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Feature Store dataset for Marketing behavioural modeling"
      friendly_name: "Feature Store"
      max_time_travel_hours: 168 
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    purchase_propensity:
      name: "purchase_propensity"
      location: "${location}"
      project_id: "${project_id}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Purchase Propensity Use Case dataset for Marketing behavioural modeling"
      friendly_name: "Purchase Propensity Dataset"
      max_time_travel_hours: 168
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    customer_lifetime_value:
      project_id: "${project_id}"
      name: "customer_lifetime_value"
      location: "${location}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Customer Lifetime Value Use Case dataset for Marketing behavioural modeling"
      friendly_name: "Customer Lifetime Value Dataset"
      max_time_travel_hours: 168
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    audience_segmentation:
      project_id: "${project_id}"
      name: "audience_segmentation"
      location: "${location}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Audience Segmentation Use Case dataset for Marketing behavioural modeling"
      friendly_name: "Audience Segmentation Dataset"
      max_time_travel_hours: 168
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    auto_audience_segmentation:
      project_id: "${project_id}"
      name: "auto_audience_segmentation"
      location: "${location}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Auto Audience Segmentation Use Case dataset for Marketing behavioural modeling"
      friendly_name: "Auto Audience Segmentation Dataset"
      max_time_travel_hours: 48
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    aggregated_vbb:
      project_id: "${project_id}"
      name: "aggregated_vbb"
      location: "${location}"
      collation: "und:ci"
      is_case_insensitive: TRUE
      description: "Aggregated VBB Use Case dataset for Marketing behavioural modeling"
      friendly_name: "Aggregated VBB Dataset"
      max_time_travel_hours: 48
      default_partition_expiration_days: 365
      default_table_expiration_days: 365
    aggregated_predictions:
      project_id: "${project_id}"
      name: "aggregated_predictions"
      location: "${location}"
      description: "Dataset with aggregated prediction results from multiple use cases"
      friendly_name: "Aggregated Predictions Dataset"
  table:
    audience_segmentation_inference_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      table_name: "audience_segmentation_inference_preparation"
      location: "${location}"
      table_description: "Audience Segmentation Inference Preparation table to be used for Model Prediction"
    customer_lifetime_value_inference_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      table_name: "customer_lifetime_value_inference_preparation"
      location: "${location}"
      table_description: "Customer Lifetime Value Inference Preparation table to be used for Model Prediction"
    customer_lifetime_value_label:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      table_name: "customer_lifetime_value_label"
      location: "${location}"
      table_description: "Customer Lifetime Value Inference Preparation table to be used for Model Prediction"
    purchase_propensity_inference_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      table_name: "purchase_propensity_inference_preparation"
      location: "${location}"
      table_description: "Purchase Propensity Inference Preparation table to be used for Model Prediction"
    purchase_propensity_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "purchase_propensity_label"
      location: "${location}"
      table_description: "Purchase Propensity Label table to be used for Model Prediction"
    user_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_dimensions"
      location: "${location}"
      table_description: "User Dimensions table as part of the Feature Store"
    user_lifetime_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_lifetime_dimensions"
      location: "${location}"
      table_description: "User Lifetime Dimensions table as part of the Feature Store"
    user_lookback_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_lookback_metrics"
      location: "${location}"
      table_description: "User Lookback Metrics table as part of the Feature Store"
    user_rolling_window_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_rolling_window_lifetime_metrics"
      location: "${location}"
      table_description: "User Rolling Window Lifetime Metrics table as part of the Feature Store" 
    user_rolling_window_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_rolling_window_metrics"
      location: "${location}"
      table_description: "User Rolling Window Metrics table as part of the Feature Store"
    user_scoped_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_scoped_lifetime_metrics"
      location: "${location}"
      table_description: "User Scoped Lifetime Metrics table as part of the Feature Store"
    user_scoped_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_scoped_metrics"
      location: "${location}"
      table_description: "User Scoped Metrics table as part of the Feature Store"
    user_scoped_segmentation_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_scoped_segmentation_metrics"
      location: "${location}"
      table_description: "User Scoped Segmentation Metrics table as part of the Feature Store" 
    user_segmentation_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_segmentation_dimensions"
      location: "${location}"
      table_description: "User Segmentation Dimensions table as part of the Feature Store"
    user_session_event_aggregated_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      table_name: "user_session_event_aggregated_metrics"
      location: "${location}"
      table_description: "User Session Event Aggregated Metrics table as part of the Feature Store" 
  query:
    audience_segmentation_query_template:
      none: none
    auto_audience_segmentation_query_template:
      none: none
    purchase_propensity_query_template:
      none: none
    cltv_query_template:
      none: none
    invoke_aggregated_value_based_bidding_training_preparation:
      project_id: "${project_id}"
      dataset: "aggregated_vbb"
      stored_procedure: "aggregated_value_based_bidding_training_preparation"
    invoke_purchase_propensity_training_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      stored_procedure: "purchase_propensity_training_preparation"
      interval_max_date: 15
      interval_min_date: 30
      train_split_end_number: 5
      validation_split_end_number: 8
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    invoke_audience_segmentation_training_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      stored_procedure: "audience_segmentation_training_preparation"
      interval_max_date: 1
      interval_min_date: 15
      train_split_end_number: 5
      validation_split_end_number: 8
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    invoke_customer_lifetime_value_training_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      stored_procedure: "customer_lifetime_value_training_preparation"
      interval_max_date: 180
      interval_min_date: 180
      train_split_end_number: 5
      validation_split_end_number: 8
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}" 
    invoke_backfill_user_lifetime_dimensions:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_lifetime_dimensions"
      interval_max_date: 180
      interval_min_date: 180
      interval_end_date: 180
    invoke_backfill_user_scoped_lifetime_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}" 
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_scoped_lifetime_metrics"
      interval_max_date: 180
      interval_min_date: 180
      interval_end_date: 180
    invoke_backfill_user_session_event_aggregated_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}" 
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_session_event_aggregated_metrics"
      interval_max_date: 15
      interval_min_date: 30
      interval_end_date: 30
    invoke_backfill_customer_lifetime_value_label:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "customer_lifetime_value_label"
      interval_max_date: 180
      interval_min_date: 180
      interval_end_date: 180
    invoke_backfill_user_lookback_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_lookback_metrics"
      interval_min_date: 15
      interval_end_date: 15
    invoke_backfill_user_rolling_window_lifetime_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_rolling_window_lifetime_metrics"
      interval_max_date: 180
      interval_min_date: 180
      interval_end_date: 180
    invoke_backfill_user_scoped_segmentation_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_scoped_segmentation_metrics"
      interval_min_date: 15
      interval_end_date: 15 
    invoke_backfill_user_segmentation_dimensions:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_segmentation_dimensions"
      interval_min_date: 15
      interval_end_date: 15
    invoke_backfill_purchase_propensity_label:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "purchase_propensity_label"
      interval_max_date: 15
      interval_min_date: 30
      interval_end_date: 30
    invoke_backfill_user_dimensions:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_dimensions"
      interval_max_date: 15
      interval_min_date: 30
      interval_end_date: 30
    invoke_backfill_user_rolling_window_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}" 
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_rolling_window_metrics"
      interval_max_date: 15
      interval_min_date: 30
      interval_end_date: 30
    invoke_backfill_user_scoped_metrics:
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      project_id: "${project_id}"
      dataset: "feature_store"
      insert_table: "user_scoped_metrics"
      interval_max_date: 15
      interval_min_date: 30
      interval_end_date: 30 
    invoke_customer_lifetime_value_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "customer_lifetime_value_label"
      interval_input_date: 180
    invoke_purchase_propensity_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "purchase_propensity_label"
      interval_input_date: 15
    invoke_user_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_dimensions"
      interval_end_date: 180
    invoke_user_lifetime_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_lifetime_dimensions"
      interval_end_date: 180 
    invoke_user_scoped_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_scoped_lifetime_metrics"
      interval_end_date: 180 
    invoke_user_session_event_aggregated_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_session_event_aggregated_metrics"
      interval_end_date: 180
    invoke_user_lookback_metrics: 
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_lookback_metrics"
      interval_end_date: 180
    invoke_user_rolling_window_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_rolling_window_lifetime_metrics"
      interval_end_date: 180
    invoke_user_scoped_segmentation_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_scoped_segmentation_metrics"
      interval_end_date: 180
    invoke_user_segmentation_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_segmentation_dimensions"
      interval_end_date: 180
    invoke_user_rolling_window_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_rolling_window_metrics"
      interval_end_date: 180
    invoke_user_scoped_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      stored_procedure: "user_scoped_metrics"
      interval_end_date: 180 
    invoke_purchase_propensity_inference_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      stored_procedure: "purchase_propensity_inference_preparation"
    invoke_customer_lifetime_value_inference_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      stored_procedure: "customer_lifetime_value_inference_preparation"
    invoke_audience_segmentation_inference_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      stored_procedure: "audience_segmentation_inference_preparation"
    invoke_auto_audience_segmentation_inference_preparation:
      project_id: "${project_id}"
      dataset: "auto_audience_segmentation"
      stored_procedure: "auto_audience_segmentation_inference_preparation"
  procedure:
    aggregated_value_based_bidding_training_preparation:
      project_id: "${project_id}"
      dataset: "aggregated_vbb"
      name: "aggregated_value_based_bidding_training_preparation"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      view_name: "aggregated_value_based_bidding_training_full_dataset"
      expiration_duration_hours: 168
    audience_segmentation_training_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      name: "audience_segmentation_training_preparation"
      insert_table: "audience_segmentation_training_full_dataset"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      expiration_duration_hours: 168 
      samples_per_split: 100000
    customer_lifetime_value_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "customer_lifetime_value_label"
      insert_table: "customer_lifetime_value_label"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    customer_lifetime_value_training_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      name: "customer_lifetime_value_training_preparation"
      insert_table: "customer_lifetime_value_training_full_dataset"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      expiration_duration_hours: 168
    purchase_propensity_label:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "purchase_propensity_label"
      insert_table: "purchase_propensity_label"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    purchase_propensity_training_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      name: "purchase_propensity_training_preparation"
      insert_table: "purchase_propensity_training_full_dataset"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
      expiration_duration_hours: 168
    user_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_dimensions"
      insert_table: "user_dimensions"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    user_lifetime_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_lifetime_dimensions"
      insert_table: "user_lifetime_dimensions"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    user_lookback_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_lookback_metrics"
      insert_table: "user_lookback_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    user_rolling_window_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_rolling_window_lifetime_metrics"
      insert_table: "user_rolling_window_lifetime_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    user_rolling_window_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_rolling_window_metrics"
      insert_table: "user_rolling_window_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    user_scoped_lifetime_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_scoped_lifetime_metrics"
      insert_table: "user_scoped_lifetime_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    user_scoped_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_scoped_metrics"
      insert_table: "user_scoped_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    user_scoped_segmentation_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_scoped_segmentation_metrics"
      insert_table: "user_scoped_segmentation_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    user_segmentation_dimensions:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_segmentation_dimensions"
      insert_table: "user_segmentation_dimensions"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    user_session_event_aggregated_metrics:
      project_id: "${project_id}"
      dataset: "feature_store"
      name: "user_session_event_aggregated_metrics"
      insert_table: "user_session_event_aggregated_metrics"
      mds_project_id: "${mds_project_id}"
      mds_dataset: "${mds_dataset}"
    purchase_propensity_inference_preparation:
      project_id: "${project_id}"
      dataset: "purchase_propensity"
      name: "purchase_propensity_inference_preparation"
      feature_store_project_id: "${project_id}" 
      feature_store_dataset: "feature_store"
      insert_table: "purchase_propensity_inference_preparation"
      expiration_duration_hours: 168
    customer_lifetime_value_inference_preparation:
      project_id: "${project_id}"
      dataset: "customer_lifetime_value"
      name: "customer_lifetime_value_inference_preparation"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      insert_table: "customer_lifetime_value_inference_preparation"
      expiration_duration_hours: 168
    audience_segmentation_inference_preparation:
      project_id: "${project_id}"
      dataset: "audience_segmentation"
      name: "audience_segmentation_inference_preparation"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      insert_table: "audience_segmentation_inference_preparation"
      expiration_duration_hours: 168
      mds_dataset: "${mds_dataset}"
    auto_audience_segmentation_inference_preparation:
      project_id: "${project_id}"
      dataset: "auto_audience_segmentation"
      name: "auto_audience_segmentation_inference_preparation"
      feature_store_project_id: "${project_id}"
      feature_store_dataset: "feature_store"
      insert_table: "auto_audience_segmentation_inference_preparation"
      expiration_duration_hours: 12
      mds_dataset: "${mds_dataset}"
    aggregate_predictions_procedure:
      project_id: "${project_id}"
      dataset_id: "aggregated_predictions"
      table_id: "latest"
      customer_lifetime_value_dataset: "customer_lifetime_value"
      purchase_propensity_dataset: "purchase_propensity"
      audience_segmentation_dataset: "audience_segmentation"

  
